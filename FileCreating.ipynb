{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuOsQfJGjB2R",
        "outputId": "e7117b98-310d-4cf3-d584-4b5081bbf45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: mimesis in /usr/local/lib/python3.11/dist-packages (18.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2025.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.0.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (19.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install psutil mimesis pandas dask dask[dataframe]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import platform\n",
        "from datetime import datetime\n",
        "\n",
        "def get_system_info():\n",
        "    print(\"Gathering system information...\")\n",
        "\n",
        "    # CPU information\n",
        "    print(\"CPU:\")\n",
        "    print(f\"Physical cores: {psutil.cpu_count(logical=False)}\")\n",
        "    print(f\"Total cores: {psutil.cpu_count(logical=True)}\")\n",
        "    cpu_freq = psutil.cpu_freq()\n",
        "    print(f\"Max Frequency: {cpu_freq.max:.2f}Mhz\")\n",
        "    print(f\"Min Frequency: {cpu_freq.min:.2f}Mhz\")\n",
        "    print(f\"Current Frequency: {cpu_freq.current:.2f}Mhz\")\n",
        "    print(f\"Total CPU Usage: {psutil.cpu_percent()}%\")\n",
        "\n",
        "    # Memory Information\n",
        "    print(\"\\nMemory:\")\n",
        "    svmem = psutil.virtual_memory()\n",
        "    print(f\"Total: {get_size(svmem.total)}\")\n",
        "    print(f\"Available: {get_size(svmem.available)}\")\n",
        "    print(f\"Used: {get_size(svmem.used)}\")\n",
        "    print(f\"Percentage: {svmem.percent}%\")\n",
        "\n",
        "    # Disk Information\n",
        "    print(\"\\nDisk Information:\")\n",
        "    partitions = psutil.disk_partitions()\n",
        "    for partition in partitions:\n",
        "        print(f\"=== Device: {partition.device} ===\")\n",
        "        print(f\"  Mountpoint: {partition.mountpoint}\")\n",
        "        print(f\"  File system type: {partition.fstype}\")\n",
        "        try:\n",
        "            partition_usage = psutil.disk_usage(partition.mountpoint)\n",
        "        except PermissionError:\n",
        "            continue\n",
        "        print(f\"  Total Size: {get_size(partition_usage.total)}\")\n",
        "        print(f\"  Used: {get_size(partition_usage.used)}\")\n",
        "        print(f\"  Free: {get_size(partition_usage.free)}\")\n",
        "        print(f\"  Percentage: {partition_usage.percent}%\")\n",
        "\n",
        "    # OS information\n",
        "    print(\"\\nOperating System:\")\n",
        "    print(f\"System: {platform.system()}\")\n",
        "    print(f\"Node Name: {platform.node()}\")\n",
        "    print(f\"Release: {platform.release()}\")\n",
        "    print(f\"Version: {platform.version()}\")\n",
        "    print(f\"Machine: {platform.machine()}\")\n",
        "    print(f\"Processor: {platform.processor()}\")\n",
        "\n",
        "def get_size(bytes, suffix=\"B\"):\n",
        "    \"\"\"\n",
        "    Scale bytes to its proper format\n",
        "    e.g.:\n",
        "        1253656 => '1.20MB'\n",
        "        1253656678 => '1.17GB'\n",
        "    \"\"\"\n",
        "    factor = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < factor:\n",
        "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
        "        bytes /= factor\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    get_system_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUrVFjG0jVRe",
        "outputId": "87d03715-7293-4bef-9f02-1890123cd928"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gathering system information...\n",
            "CPU:\n",
            "Physical cores: 48\n",
            "Total cores: 96\n",
            "Max Frequency: 0.00Mhz\n",
            "Min Frequency: 0.00Mhz\n",
            "Current Frequency: 2000.17Mhz\n",
            "Total CPU Usage: 1.2%\n",
            "\n",
            "Memory:\n",
            "Total: 334.56GB\n",
            "Available: 330.02GB\n",
            "Used: 2.35GB\n",
            "Percentage: 1.4%\n",
            "\n",
            "Disk Information:\n",
            "=== Device: /dev/root ===\n",
            "  Mountpoint: /usr/sbin/docker-init\n",
            "  File system type: ext2\n",
            "  Total Size: 1.93GB\n",
            "  Used: 1.13GB\n",
            "  Free: 819.42MB\n",
            "  Percentage: 58.5%\n",
            "=== Device: /dev/sda1 ===\n",
            "  Mountpoint: /etc/resolv.conf\n",
            "  File system type: ext4\n",
            "  Total Size: 232.07GB\n",
            "  Used: 28.62GB\n",
            "  Free: 203.43GB\n",
            "  Percentage: 12.3%\n",
            "=== Device: /dev/sda1 ===\n",
            "  Mountpoint: /etc/hostname\n",
            "  File system type: ext4\n",
            "  Total Size: 232.07GB\n",
            "  Used: 28.62GB\n",
            "  Free: 203.43GB\n",
            "  Percentage: 12.3%\n",
            "=== Device: /dev/sda1 ===\n",
            "  Mountpoint: /etc/hosts\n",
            "  File system type: ext4\n",
            "  Total Size: 232.07GB\n",
            "  Used: 28.62GB\n",
            "  Free: 203.43GB\n",
            "  Percentage: 12.3%\n",
            "\n",
            "Operating System:\n",
            "System: Linux\n",
            "Node Name: f6015db84b51\n",
            "Release: 6.1.85+\n",
            "Version: #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "Machine: x86_64\n",
            "Processor: x86_64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "import threading\n",
        "import psutil\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "from mimesis import Person, Address, Datetime\n",
        "from mimesis.enums import Gender\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def monitor_resources(interval, stats):\n",
        "    while not stats['stop']:\n",
        "        cpu = psutil.cpu_percent(interval=None)\n",
        "        memory = psutil.virtual_memory().percent\n",
        "        stats['cpu'].append(cpu)\n",
        "        stats['memory'].append(memory)\n",
        "        time.sleep(interval)\n",
        "\n",
        "def print_resource_stats(stats):\n",
        "    print(f\"Average CPU Usage: {sum(stats['cpu']) / len(stats['cpu']):.2f}%\")\n",
        "    print(f\"Max CPU Usage: {max(stats['cpu']):.2f}%\")\n",
        "    print(f\"Min CPU Usage: {min(stats['cpu']):.2f}%\")\n",
        "    print(f\"Average Memory Usage: {sum(stats['memory']) / len(stats['memory']):.2f}%\")\n",
        "    print(f\"Max Memory Usage: {max(stats['memory']):.2f}%\")\n",
        "    print(f\"Min Memory Usage: {min(stats['memory']):.2f}%\")\n",
        "\n",
        "def generate_data(row_count):\n",
        "    person = Person('en')\n",
        "    address = Address('en')\n",
        "    datetime = Datetime('en')\n",
        "    return [{\n",
        "        \"Name\": person.full_name(gender=random.choice([Gender.MALE, Gender.FEMALE])),\n",
        "        \"Email\": person.email(),\n",
        "        \"Address\": address.address(),\n",
        "        \"Phone\": person.telephone(),\n",
        "        \"Date of Birth\": datetime.date().isoformat(),\n",
        "        \"Gender\": random.choice([\"Male\", \"Female\"]),\n",
        "        \"Company\": address.city() + \" Corp\",\n",
        "        \"Position\": person.occupation(),\n",
        "        \"Salary\": round(random.uniform(30000, 200000), 2),\n",
        "        \"Retired\": random.choice([\"Yes\", \"No\"])\n",
        "    } for _ in range(row_count)]\n",
        "\n",
        "def remove_test_files(directory, patterns):\n",
        "    os.chdir(directory)\n",
        "    for pattern in patterns:\n",
        "        for file in glob.glob(pattern):\n",
        "            try:\n",
        "                os.remove(file)\n",
        "                print(f\"Removed file: {file}\")\n",
        "            except OSError as e:\n",
        "                print(f\"Error: {file} : {e.strerror}\")\n",
        "\n",
        "def row_by_row_approach(num_rows):\n",
        "    print(\"Starting Row-by-row Approach\")\n",
        "    with open('row_by_row.csv', 'w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=generate_data(1)[0].keys())\n",
        "        writer.writeheader()\n",
        "        for data in generate_data(num_rows):\n",
        "            writer.writerow(data)\n",
        "\n",
        "def dataframe_approach(num_rows):\n",
        "    print(\"Starting DataFrame Approach\")\n",
        "    df = pd.DataFrame(generate_data(num_rows))\n",
        "    df.to_csv('dataframe.csv', index=False)\n",
        "\n",
        "def streaming_chunks_approach(num_rows, chunk_size):\n",
        "    print(\"Starting Streaming Chunks Approach\")\n",
        "    with open('streaming_chunks.csv', 'w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=generate_data(1)[0].keys())\n",
        "        writer.writeheader()\n",
        "        num_chunks = (num_rows + chunk_size - 1) // chunk_size\n",
        "        for _ in range(num_chunks):\n",
        "            writer.writerows(generate_data(min(chunk_size, num_rows)))\n",
        "            num_rows -= chunk_size\n",
        "\n",
        "def parallel_processing_approach(num_rows, num_processes):\n",
        "    print(\"Starting Parallel Processing Approach with\", num_processes, \"processes\")\n",
        "    pool = Pool(num_processes)\n",
        "    chunk_size = num_rows // num_processes\n",
        "    chunks = [generate_data(chunk_size + (1 if i < num_rows % num_processes else 0)) for i in range(num_processes)]\n",
        "    pool.starmap(worker, [(chunk, i) for i, chunk in enumerate(chunks)])\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    consolidate_files('parallel_output_*.csv', 'final_parallel_output.csv')\n",
        "\n",
        "def dask_approach(num_rows, npartitions, output_filename):\n",
        "    print(\"Starting Dask Approach with\", npartitions, \"partitions\")\n",
        "    ddf = dd.from_pandas(pd.DataFrame(generate_data(num_rows)), npartitions=npartitions)\n",
        "    temp_output = 'temp_dask_output_*.csv'\n",
        "    ddf.to_csv(temp_output, index=False)\n",
        "    with ProgressBar():\n",
        "        consolidate_files('temp_dask_output_*.csv', output_filename)\n",
        "\n",
        "def consolidate_files(file_pattern, output_file):\n",
        "    start_time = time.time()\n",
        "    files = glob.glob(file_pattern)\n",
        "    with open(output_file, 'w', newline='') as final_file:\n",
        "        final_writer = csv.writer(final_file)\n",
        "        for i, file_path in enumerate(files):\n",
        "            with open(file_path, 'r') as source_file:\n",
        "                reader = csv.reader(source_file)\n",
        "                if i != 0:\n",
        "                    next(reader)  # Skip header for all but the first file\n",
        "                final_writer.writerows(reader)\n",
        "            os.remove(file_path)\n",
        "    end_time = time.time()\n",
        "    print(f\"Consolidation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "def process_approach(approach_func, num_rows, *args):\n",
        "    stats = {'cpu': [], 'memory': [], 'stop': False}\n",
        "    monitor_thread = threading.Thread(target=monitor_resources, args=(10, stats))\n",
        "    monitor_thread.start()\n",
        "\n",
        "    start_time = time.time()\n",
        "    approach_func(num_rows, *args)\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    stats['stop'] = True\n",
        "    monitor_thread.join()\n",
        "\n",
        "    print(f\"{approach_func.__name__} completed in {duration:.2f} seconds.\")\n",
        "    print_resource_stats(stats)\n",
        "\n",
        "def main():\n",
        "    directory = './'\n",
        "    patterns = ['row_by_row.csv', 'dataframe.csv', 'streaming_chunks.csv',\n",
        "                'parallel_output_*.csv', 'final_parallel_output.csv',\n",
        "                'temp_dask_output_*.csv', 'final_dask_output.csv']\n",
        "    remove_test_files(directory, patterns)\n",
        "    num_rows = 100000000\n",
        "    chunk_size = 100000\n",
        "    num_cores = psutil.cpu_count(logical=True)\n",
        "    process_approach(row_by_row_approach, num_rows)\n",
        "    process_approach(dataframe_approach, num_rows)\n",
        "    process_approach(streaming_chunks_approach, num_rows, chunk_size)\n",
        "    process_approach(parallel_processing_approach, num_rows, num_cores)\n",
        "    process_approach(dask_approach, num_rows, num_cores, 'final_dask_output.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIFzHjVdjXHA",
        "outputId": "53c0694e-acdb-4cee-c811-7bddcf4f7220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DataFrame Approach\n"
          ]
        }
      ]
    }
  ]
}
