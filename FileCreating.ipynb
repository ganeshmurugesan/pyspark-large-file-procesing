{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG61GZSnh8Zo",
        "outputId": "f2b5b2f6-e145-4ef0-cac6-88b7e5cb6540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: mimesis in /usr/local/lib/python3.11/dist-packages (18.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2025.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.0.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (19.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install psutil mimesis pandas dask dask[dataframe]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "import threading\n",
        "import psutil\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "from mimesis import Person, Address, Datetime\n",
        "from mimesis.enums import Gender\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def monitor_resources(interval, stats):\n",
        "    while not stats['stop']:\n",
        "        cpu = psutil.cpu_percent(interval=None)\n",
        "        memory = psutil.virtual_memory().percent\n",
        "        stats['cpu'].append(cpu)\n",
        "        stats['memory'].append(memory)\n",
        "        time.sleep(interval)\n",
        "\n",
        "def print_resource_stats(stats):\n",
        "    print(f\"Average CPU Usage: {sum(stats['cpu']) / len(stats['cpu']):.2f}%\")\n",
        "    print(f\"Max CPU Usage: {max(stats['cpu']):.2f}%\")\n",
        "    print(f\"Min CPU Usage: {min(stats['cpu']):.2f}%\")\n",
        "    print(f\"Average Memory Usage: {sum(stats['memory']) / len(stats['memory']):.2f}%\")\n",
        "    print(f\"Max Memory Usage: {max(stats['memory']):.2f}%\")\n",
        "    print(f\"Min Memory Usage: {min(stats['memory']):.2f}%\")\n",
        "\n",
        "def generate_data(row_count):\n",
        "    person = Person('en')\n",
        "    address = Address('en')\n",
        "    datetime = Datetime('en')\n",
        "    return [{\n",
        "        \"Name\": person.full_name(gender=random.choice([Gender.MALE, Gender.FEMALE])),\n",
        "        \"Email\": person.email(),\n",
        "        \"Address\": address.address(),\n",
        "        \"Phone\": person.telephone(),\n",
        "        \"Date of Birth\": datetime.date().isoformat(),\n",
        "        \"Gender\": random.choice([\"Male\", \"Female\"]),\n",
        "        \"Company\": address.city() + \" Corp\",\n",
        "        \"Position\": person.occupation(),\n",
        "        \"Salary\": round(random.uniform(30000, 200000), 2),\n",
        "        \"Retired\": random.choice([\"Yes\", \"No\"])\n",
        "    } for _ in range(row_count)]\n",
        "\n",
        "def remove_test_files(directory, patterns):\n",
        "    os.chdir(directory)\n",
        "    for pattern in patterns:\n",
        "        for file in glob.glob(pattern):\n",
        "            try:\n",
        "                os.remove(file)\n",
        "                print(f\"Removed file: {file}\")\n",
        "            except OSError as e:\n",
        "                print(f\"Error: {file} : {e.strerror}\")\n",
        "\n",
        "def row_by_row_approach(num_rows):\n",
        "    print(\"Starting Row-by-row Approach\")\n",
        "    with open('row_by_row.csv', 'w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=generate_data(1)[0].keys())\n",
        "        writer.writeheader()\n",
        "        for data in generate_data(num_rows):\n",
        "            writer.writerow(data)\n",
        "\n",
        "def dataframe_approach(num_rows):\n",
        "    print(\"Starting DataFrame Approach\")\n",
        "    df = pd.DataFrame(generate_data(num_rows))\n",
        "    df.to_csv('dataframe.csv', index=False)\n",
        "\n",
        "def streaming_chunks_approach(num_rows, chunk_size):\n",
        "    print(\"Starting Streaming Chunks Approach\")\n",
        "    with open('streaming_chunks.csv', 'w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=generate_data(1)[0].keys())\n",
        "        writer.writeheader()\n",
        "        num_chunks = (num_rows + chunk_size - 1) // chunk_size\n",
        "        for _ in range(num_chunks):\n",
        "            writer.writerows(generate_data(min(chunk_size, num_rows)))\n",
        "            num_rows -= chunk_size\n",
        "\n",
        "def worker(data_chunk, index):\n",
        "    file_name = f'parallel_output_{index}.csv'\n",
        "    with open(file_name, 'w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=data_chunk[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data_chunk)\n",
        "\n",
        "def parallel_processing_approach(num_rows, num_processes):\n",
        "    print(\"Starting Parallel Processing Approach with\", num_processes, \"processes\")\n",
        "    pool = Pool(num_processes)\n",
        "    chunk_size = num_rows // num_processes\n",
        "    chunks = [generate_data(chunk_size + (1 if i < num_rows % num_processes else 0)) for i in range(num_processes)]\n",
        "    pool.starmap(worker, [(chunk, i) for i, chunk in enumerate(chunks)])\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    consolidate_files('parallel_output_*.csv', 'final_parallel_output.csv')\n",
        "\n",
        "def dask_approach(num_rows, npartitions, output_filename):\n",
        "    print(\"Starting Dask Approach with\", npartitions, \"partitions\")\n",
        "    ddf = dd.from_pandas(pd.DataFrame(generate_data(num_rows)), npartitions=npartitions)\n",
        "    temp_output = 'temp_dask_output_*.csv'\n",
        "    ddf.to_csv(temp_output, index=False)\n",
        "    with ProgressBar():\n",
        "        consolidate_files('temp_dask_output_*.csv', output_filename)\n",
        "\n",
        "def consolidate_files(file_pattern, output_file):\n",
        "    start_time = time.time()\n",
        "    files = glob.glob(file_pattern)\n",
        "    with open(output_file, 'w', newline='') as final_file:\n",
        "        final_writer = csv.writer(final_file)\n",
        "        for i, file_path in enumerate(files):\n",
        "            with open(file_path, 'r') as source_file:\n",
        "                reader = csv.reader(source_file)\n",
        "                if i != 0:\n",
        "                    next(reader)  # Skip header for all but the first file\n",
        "                final_writer.writerows(reader)\n",
        "            os.remove(file_path)\n",
        "    end_time = time.time()\n",
        "    print(f\"Consolidation completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "def process_approach(approach_func, num_rows, *args):\n",
        "    stats = {'cpu': [], 'memory': [], 'stop': False}\n",
        "    monitor_thread = threading.Thread(target=monitor_resources, args=(10, stats))\n",
        "    monitor_thread.start()\n",
        "\n",
        "    start_time = time.time()\n",
        "    approach_func(num_rows, *args)\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    stats['stop'] = True\n",
        "    monitor_thread.join()\n",
        "\n",
        "    print(f\"{approach_func.__name__} completed in {duration:.2f} seconds.\")\n",
        "    print_resource_stats(stats)\n",
        "\n",
        "def main():\n",
        "    directory = './'\n",
        "    patterns = ['row_by_row.csv', 'dataframe.csv', 'streaming_chunks.csv',\n",
        "                'parallel_output_*.csv', 'final_parallel_output.csv',\n",
        "                'temp_dask_output_*.csv', 'final_dask_output.csv']\n",
        "\n",
        "    # List of num_rows and chunk_size combinations\n",
        "    combinations = [\n",
        "        (10000000, 100000),\n",
        "        (1000000, 10000),\n",
        "        (100000, 5000),\n",
        "        (10000, 1000),\n",
        "        (1000, 100),\n",
        "        (100, 10)\n",
        "    ]\n",
        "\n",
        "    num_cores = psutil.cpu_count(logical=True)  # Determine the number of cores available\n",
        "\n",
        "    for num_rows, chunk_size in combinations:\n",
        "        # Remove existing files to ensure a clean start\n",
        "        remove_test_files(directory, patterns)\n",
        "\n",
        "        print(f\"Processing {num_rows} rows with chunk size {chunk_size}:\")\n",
        "\n",
        "        # Execute each approach with the current combination of num_rows and chunk_size\n",
        "        process_approach(row_by_row_approach, num_rows)\n",
        "        process_approach(dataframe_approach, num_rows)\n",
        "        process_approach(streaming_chunks_approach, num_rows, chunk_size)\n",
        "        process_approach(parallel_processing_approach, num_rows, num_cores)\n",
        "        process_approach(dask_approach, num_rows, num_cores, 'final_dask_output.csv')\n",
        "        print(\"\\n\")  # Adds a newline for better separation of output for each combination\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBBLAaroh_mn",
        "outputId": "0e51e311-9942-4e84-caf4-cf11096eab93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 10000000 rows with chunk size 100000:\n",
            "Starting Row-by-row Approach\n",
            "row_by_row_approach completed in 284.78 seconds.\n",
            "Average CPU Usage: 1.56%\n",
            "Max CPU Usage: 1.80%\n",
            "Min CPU Usage: 1.20%\n",
            "Average Memory Usage: 2.57%\n",
            "Max Memory Usage: 3.40%\n",
            "Min Memory Usage: 1.40%\n",
            "Starting DataFrame Approach\n",
            "dataframe_approach completed in 298.20 seconds.\n",
            "Average CPU Usage: 1.51%\n",
            "Max CPU Usage: 1.70%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 3.90%\n",
            "Min Memory Usage: 1.60%\n",
            "Starting Streaming Chunks Approach\n",
            "streaming_chunks_approach completed in 280.21 seconds.\n",
            "Average CPU Usage: 1.57%\n",
            "Max CPU Usage: 1.80%\n",
            "Min CPU Usage: 1.20%\n",
            "Average Memory Usage: 1.70%\n",
            "Max Memory Usage: 1.70%\n",
            "Min Memory Usage: 1.70%\n",
            "Starting Parallel Processing Approach with 96 processes\n",
            "Consolidation completed in 50.32 seconds.\n",
            "parallel_processing_approach completed in 325.37 seconds.\n",
            "Average CPU Usage: 2.24%\n",
            "Max CPU Usage: 5.10%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 3.29%\n",
            "Max Memory Usage: 6.00%\n",
            "Min Memory Usage: 1.70%\n",
            "Starting Dask Approach with 96 partitions\n",
            "Consolidation completed in 53.81 seconds.\n",
            "dask_approach completed in 407.60 seconds.\n",
            "Average CPU Usage: 1.69%\n",
            "Max CPU Usage: 2.30%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 3.28%\n",
            "Max Memory Usage: 4.40%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "\n",
            "Removed file: row_by_row.csv\n",
            "Removed file: dataframe.csv\n",
            "Removed file: streaming_chunks.csv\n",
            "Removed file: final_parallel_output.csv\n",
            "Removed file: final_dask_output.csv\n",
            "Processing 1000000 rows with chunk size 10000:\n",
            "Starting Row-by-row Approach\n",
            "row_by_row_approach completed in 28.21 seconds.\n",
            "Average CPU Usage: 1.30%\n",
            "Max CPU Usage: 1.50%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 2.83%\n",
            "Max Memory Usage: 2.90%\n",
            "Min Memory Usage: 2.80%\n",
            "Starting DataFrame Approach\n",
            "dataframe_approach completed in 29.57 seconds.\n",
            "Average CPU Usage: 1.43%\n",
            "Max CPU Usage: 1.50%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Streaming Chunks Approach\n",
            "streaming_chunks_approach completed in 27.95 seconds.\n",
            "Average CPU Usage: 1.43%\n",
            "Max CPU Usage: 1.50%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Parallel Processing Approach with 96 processes\n",
            "Consolidation completed in 5.06 seconds.\n",
            "parallel_processing_approach completed in 43.13 seconds.\n",
            "Average CPU Usage: 2.30%\n",
            "Max CPU Usage: 4.20%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 3.22%\n",
            "Max Memory Usage: 3.60%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Dask Approach with 96 partitions\n",
            "Consolidation completed in 4.87 seconds.\n",
            "dask_approach completed in 39.25 seconds.\n",
            "Average CPU Usage: 1.38%\n",
            "Max CPU Usage: 1.60%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "\n",
            "\n",
            "Removed file: row_by_row.csv\n",
            "Removed file: dataframe.csv\n",
            "Removed file: streaming_chunks.csv\n",
            "Removed file: final_parallel_output.csv\n",
            "Removed file: final_dask_output.csv\n",
            "Processing 100000 rows with chunk size 5000:\n",
            "Starting Row-by-row Approach\n",
            "row_by_row_approach completed in 2.82 seconds.\n",
            "Average CPU Usage: 1.30%\n",
            "Max CPU Usage: 1.30%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting DataFrame Approach\n",
            "dataframe_approach completed in 2.96 seconds.\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Streaming Chunks Approach\n",
            "streaming_chunks_approach completed in 2.82 seconds.\n",
            "Average CPU Usage: 0.80%\n",
            "Max CPU Usage: 0.80%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Parallel Processing Approach with 96 processes\n",
            "Consolidation completed in 0.50 seconds.\n",
            "parallel_processing_approach completed in 13.30 seconds.\n",
            "Average CPU Usage: 1.65%\n",
            "Max CPU Usage: 2.60%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 3.10%\n",
            "Max Memory Usage: 3.50%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Dask Approach with 96 partitions\n",
            "Consolidation completed in 0.49 seconds.\n",
            "dask_approach completed in 4.31 seconds.\n",
            "Average CPU Usage: 1.70%\n",
            "Max CPU Usage: 1.70%\n",
            "Min CPU Usage: 1.70%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "\n",
            "\n",
            "Removed file: row_by_row.csv\n",
            "Removed file: dataframe.csv\n",
            "Removed file: streaming_chunks.csv\n",
            "Removed file: final_parallel_output.csv\n",
            "Removed file: final_dask_output.csv\n",
            "Processing 10000 rows with chunk size 1000:\n",
            "Starting Row-by-row Approach\n",
            "row_by_row_approach completed in 0.29 seconds.\n",
            "Average CPU Usage: 0.90%\n",
            "Max CPU Usage: 0.90%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting DataFrame Approach\n",
            "dataframe_approach completed in 0.30 seconds.\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Streaming Chunks Approach\n",
            "streaming_chunks_approach completed in 0.30 seconds.\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Parallel Processing Approach with 96 processes\n",
            "Consolidation completed in 0.06 seconds.\n",
            "parallel_processing_approach completed in 10.36 seconds.\n",
            "Average CPU Usage: 1.90%\n",
            "Max CPU Usage: 3.30%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.80%\n",
            "Max Memory Usage: 2.90%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Dask Approach with 96 partitions\n",
            "Consolidation completed in 0.05 seconds.\n",
            "dask_approach completed in 0.84 seconds.\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "\n",
            "\n",
            "Removed file: row_by_row.csv\n",
            "Removed file: dataframe.csv\n",
            "Removed file: streaming_chunks.csv\n",
            "Removed file: final_parallel_output.csv\n",
            "Removed file: final_dask_output.csv\n",
            "Processing 1000 rows with chunk size 100:\n",
            "Starting Row-by-row Approach\n",
            "row_by_row_approach completed in 0.03 seconds.\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting DataFrame Approach\n",
            "dataframe_approach completed in 0.04 seconds.\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Streaming Chunks Approach\n",
            "streaming_chunks_approach completed in 0.06 seconds.\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Parallel Processing Approach with 96 processes\n",
            "Consolidation completed in 0.01 seconds.\n",
            "parallel_processing_approach completed in 10.30 seconds.\n",
            "Average CPU Usage: 1.70%\n",
            "Max CPU Usage: 3.10%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.80%\n",
            "Max Memory Usage: 2.90%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Dask Approach with 96 partitions\n",
            "Consolidation completed in 0.01 seconds.\n",
            "dask_approach completed in 0.59 seconds.\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "\n",
            "\n",
            "Removed file: row_by_row.csv\n",
            "Removed file: dataframe.csv\n",
            "Removed file: streaming_chunks.csv\n",
            "Removed file: final_parallel_output.csv\n",
            "Removed file: final_dask_output.csv\n",
            "Processing 100 rows with chunk size 10:\n",
            "Starting Row-by-row Approach\n",
            "row_by_row_approach completed in 0.01 seconds.\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting DataFrame Approach\n",
            "dataframe_approach completed in 0.01 seconds.\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Streaming Chunks Approach\n",
            "streaming_chunks_approach completed in 0.02 seconds.\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Parallel Processing Approach with 96 processes\n",
            "Consolidation completed in 0.01 seconds.\n",
            "parallel_processing_approach completed in 10.10 seconds.\n",
            "Average CPU Usage: 1.60%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "Starting Dask Approach with 96 partitions\n",
            "Consolidation completed in 0.01 seconds.\n",
            "dask_approach completed in 0.48 seconds.\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.70%\n",
            "Max Memory Usage: 2.70%\n",
            "Min Memory Usage: 2.70%\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}