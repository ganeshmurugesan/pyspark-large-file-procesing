{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# System-level dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y postgresql postgresql-contrib python3-dev libpq-dev\n",
        "\n",
        "# Python packages\n",
        "!pip install pandas numpy sqlalchemy psutil mimesis dask \"dask[dataframe]\" tqdm psycopg2-binary"
      ],
      "metadata": {
        "id": "QS2fUsNN4WRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde28d93-7aa4-4e43-d4f6-0cb38999080e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "postgresql is already the newest version (14+238).\n",
            "postgresql-contrib is already the newest version (14+238).\n",
            "libpq-dev is already the newest version (14.15-0ubuntu0.22.04.1).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.38)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: mimesis in /usr/local/lib/python3.11/dist-packages (18.0.0)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2025.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.11/dist-packages (2.9.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.12.2)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.0.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (19.0.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "from typing import List, Dict\n",
        "from sqlalchemy import create_engine, text\n",
        "from multiprocessing import Pool\n",
        "import dask.dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "from mimesis.locales import Locale\n",
        "from mimesis.schema import Fieldset\n",
        "import tempfile\n",
        "import io\n",
        "\n",
        "# Docker setup for PostgreSQL\n",
        "def setup_postgres_colab():\n",
        "    \"\"\"Setup PostgreSQL in Google Colab\"\"\"\n",
        "    print(\"Setting up PostgreSQL in Google Colab...\")\n",
        "\n",
        "    # Install PostgreSQL\n",
        "    !apt-get update\n",
        "    !apt-get install -y postgresql postgresql-contrib\n",
        "\n",
        "    # Start PostgreSQL service\n",
        "    !service postgresql start\n",
        "\n",
        "    # Configure PostgreSQL to accept connections\n",
        "    !sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'password';\"\n",
        "    !sudo -u postgres psql -c \"CREATE DATABASE employees;\"\n",
        "\n",
        "    # Update pg_hba.conf to allow local connections\n",
        "    !echo \"host all all 0.0.0.0/0 md5\" | sudo tee -a /etc/postgresql/*/main/pg_hba.conf\n",
        "\n",
        "    # Update postgresql.conf to listen on all addresses\n",
        "    !echo \"listen_addresses = '*'\" | sudo tee -a /etc/postgresql/*/main/postgresql.conf\n",
        "\n",
        "    # Restart PostgreSQL to apply changes\n",
        "    !service postgresql restart\n",
        "\n",
        "    # Wait for PostgreSQL to be ready\n",
        "    connection_string = \"postgresql://postgres:password@localhost:5432/employees\"\n",
        "    engine = create_engine(connection_string)\n",
        "\n",
        "    max_attempts = 30\n",
        "    attempt = 0\n",
        "    while attempt < max_attempts:\n",
        "        try:\n",
        "            print(f\"Attempting to connect to database... (Attempt {attempt + 1}/{max_attempts})\")\n",
        "            with engine.connect() as connection:\n",
        "                connection.execute(text(\"SELECT 1\"))\n",
        "            print(\"Successfully connected to PostgreSQL!\")\n",
        "            return connection_string\n",
        "        except Exception as e:\n",
        "            print(f\"Connection attempt failed: {str(e)}\")\n",
        "            attempt += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    raise Exception(\"Failed to connect to PostgreSQL after maximum attempts\")\n",
        "\n",
        "# Database schema setup\n",
        "def setup_database(engine):\n",
        "    \"\"\"Create SCD Type 2 table schema\"\"\"\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS employees (\n",
        "                employee_id INTEGER,\n",
        "                name VARCHAR(100),\n",
        "                email VARCHAR(100),\n",
        "                address TEXT,\n",
        "                phone VARCHAR(50),\n",
        "                date_of_birth DATE,\n",
        "                gender VARCHAR(10),\n",
        "                company VARCHAR(100),\n",
        "                position VARCHAR(100),\n",
        "                salary DECIMAL(10,2),\n",
        "                retired VARCHAR(3),\n",
        "                valid_from TIMESTAMP,\n",
        "                valid_to TIMESTAMP,\n",
        "                is_current BOOLEAN,\n",
        "                PRIMARY KEY (employee_id, valid_from)\n",
        "            )\n",
        "        \"\"\"))\n",
        "        conn.commit()\n",
        "\n",
        "def generate_data(row_count: int) -> pd.DataFrame:\n",
        "    \"\"\"Generate synthetic data using Mimesis Fieldset\"\"\"\n",
        "    fieldset = Fieldset(locale=Locale.EN)\n",
        "\n",
        "    # Generate all fields at once using Fieldset\n",
        "    employee_ids = list(range(row_count))\n",
        "    names = fieldset(\"full_name\", i=row_count)\n",
        "    emails = fieldset(\"email\", i=row_count)\n",
        "    addresses = fieldset(\"address\", i=row_count)\n",
        "    phones = fieldset(\"telephone\", i=row_count)\n",
        "    dates = [str(date.isoformat()) for date in fieldset(\"date\", start=1950, end=2005, i=row_count)]\n",
        "    genders = np.random.choice([\"Male\", \"Female\"], size=row_count).tolist()\n",
        "    cities = fieldset(\"city\", i=row_count)\n",
        "    positions = fieldset(\"occupation\", i=row_count)\n",
        "    salaries = np.round(np.random.uniform(30000, 200000, row_count), 2).tolist()\n",
        "    retired = np.random.choice([\"Yes\", \"No\"], size=row_count).tolist()\n",
        "\n",
        "    # Create records using list comprehension with zip\n",
        "    records = [\n",
        "        {\n",
        "            \"employee_id\": emp_id,\n",
        "            \"name\": name,\n",
        "            \"email\": email,\n",
        "            \"address\": address,\n",
        "            \"phone\": phone,\n",
        "            \"date_of_birth\": dob,\n",
        "            \"gender\": gender,\n",
        "            \"company\": f\"{city} Corp\",\n",
        "            \"position\": position,\n",
        "            \"salary\": salary,\n",
        "            \"retired\": retired_status\n",
        "        }\n",
        "        for emp_id, name, email, address, phone, dob, gender, city, position, salary, retired_status\n",
        "        in zip(employee_ids, names, emails, addresses, phones, dates, genders,\n",
        "               cities, positions, salaries, retired)\n",
        "    ]\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def identify_changes(new_df: pd.DataFrame, engine) -> pd.DataFrame:\n",
        "    \"\"\"Compare new data with existing records and identify changes\"\"\"\n",
        "    current_records = pd.read_sql(\n",
        "        \"\"\"\n",
        "        SELECT * FROM employees\n",
        "        WHERE is_current = true\n",
        "        \"\"\",\n",
        "        engine\n",
        "    )\n",
        "\n",
        "    if len(current_records) > 0:\n",
        "        merged = new_df.merge(\n",
        "            current_records,\n",
        "            on='employee_id',\n",
        "            how='left',\n",
        "            suffixes=('_new', '_current')\n",
        "        )\n",
        "\n",
        "        changed_mask = (\n",
        "            (merged['name_new'] != merged['name_current']) |\n",
        "            (merged['email_new'] != merged['email_current']) |\n",
        "            (merged['address_new'] != merged['address_current']) |\n",
        "            (merged['phone_new'] != merged['phone_current']) |\n",
        "            (merged['position_new'] != merged['position_current']) |\n",
        "            (merged['salary_new'] != merged['salary_current'])\n",
        "        )\n",
        "\n",
        "        new_mask = merged['name_current'].isna()\n",
        "\n",
        "        new_df['change_type'] = 'no_change'\n",
        "        new_df.loc[new_mask, 'change_type'] = 'insert'\n",
        "        new_df.loc[changed_mask & ~new_mask, 'change_type'] = 'update'\n",
        "    else:\n",
        "        new_df['change_type'] = 'insert'\n",
        "\n",
        "    return new_df\n",
        "\n",
        "def apply_scd2_changes_fixed(df: pd.DataFrame, engine) -> pd.DataFrame:\n",
        "    \"\"\"Apply SCD Type 2 changes to the data with a valid PostgreSQL future date\"\"\"\n",
        "    current_timestamp = datetime.now().isoformat()\n",
        "\n",
        "    df['valid_from'] = current_timestamp\n",
        "    # Use 2099-12-31 instead of 9999-12-31 for valid_to as \"infinity\"\n",
        "    df['valid_to'] = '2099-12-31 23:59:59'\n",
        "    df['is_current'] = True\n",
        "\n",
        "    updates = df[df['change_type'] == 'update']\n",
        "    if not updates.empty:\n",
        "        with engine.begin() as conn:\n",
        "            employee_ids = tuple(updates['employee_id'].tolist())\n",
        "            conn.execute(\n",
        "                text(\"\"\"\n",
        "                    UPDATE employees\n",
        "                    SET valid_to = :valid_to,\n",
        "                        is_current = FALSE\n",
        "                    WHERE employee_id IN :employee_ids\n",
        "                    AND is_current = TRUE\n",
        "                \"\"\"),\n",
        "                {\n",
        "                    \"valid_to\": current_timestamp,\n",
        "                    \"employee_ids\": employee_ids\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return df.drop(columns=['change_type'])\n",
        "\n",
        "def reset_table(engine):\n",
        "    \"\"\"Drop and recreate the employees table instead of truncating\"\"\"\n",
        "    try:\n",
        "        with engine.connect() as conn:\n",
        "            conn.execute(text(\"DROP TABLE IF EXISTS employees\"))\n",
        "            conn.commit()\n",
        "\n",
        "        # Recreate the table schema\n",
        "        setup_database(engine)\n",
        "\n",
        "        print(\"Table dropped and recreated successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error resetting table: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Modify truncate_table to use reset_table instead\n",
        "def truncate_table(engine):\n",
        "    \"\"\"Use reset_table instead of truncate for cleaner state\"\"\"\n",
        "    reset_table(engine)\n",
        "\n",
        "# Resource monitoring\n",
        "def monitor_resources(interval, stats):\n",
        "    \"\"\"Monitor CPU and memory usage\"\"\"\n",
        "    while not stats['stop']:\n",
        "        stats['cpu'].append(psutil.cpu_percent(interval=None))\n",
        "        stats['memory'].append(psutil.virtual_memory().percent)\n",
        "        time.sleep(interval)\n",
        "\n",
        "def print_resource_stats(stats):\n",
        "    \"\"\"Print resource usage statistics\"\"\"\n",
        "    print(\"\\nResource Usage Statistics:\")\n",
        "    print(f\"Average CPU Usage: {sum(stats['cpu']) / len(stats['cpu']):.2f}%\")\n",
        "    print(f\"Max CPU Usage: {max(stats['cpu']):.2f}%\")\n",
        "    print(f\"Min CPU Usage: {min(stats['cpu']):.2f}%\")\n",
        "    print(f\"Average Memory Usage: {sum(stats['memory']) / len(stats['memory']):.2f}%\")\n",
        "    print(f\"Max Memory Usage: {max(stats['memory']):.2f}%\")\n",
        "    print(f\"Min Memory Usage: {min(stats['memory']):.2f}%\")\n",
        "\n",
        "def monitor_performance(func):\n",
        "    \"\"\"Decorator to monitor performance of loading methods\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        stats = {'cpu': [], 'memory': [], 'stop': False}\n",
        "\n",
        "        # Start monitoring thread\n",
        "        monitor_thread = threading.Thread(target=monitor_resources, args=(1, stats))\n",
        "        monitor_thread.start()\n",
        "\n",
        "        try:\n",
        "            # Execute the loading function\n",
        "            start_time = time.time()\n",
        "            func(*args, **kwargs)\n",
        "            duration = time.time() - start_time\n",
        "\n",
        "            # Stop monitoring\n",
        "            stats['stop'] = True\n",
        "            monitor_thread.join()\n",
        "\n",
        "            # Calculate resource statistics\n",
        "            resource_stats = {\n",
        "                'duration': duration,\n",
        "                'avg_cpu': sum(stats['cpu']) / len(stats['cpu']) if stats['cpu'] else 0,\n",
        "                'max_cpu': max(stats['cpu']) if stats['cpu'] else 0,\n",
        "                'avg_memory': sum(stats['memory']) / len(stats['memory']) if stats['memory'] else 0,\n",
        "                'max_memory': max(stats['memory']) if stats['memory'] else 0\n",
        "            }\n",
        "\n",
        "            print_resource_stats(stats)\n",
        "            return duration, resource_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            stats['stop'] = True\n",
        "            monitor_thread.join()\n",
        "            raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_row_by_row(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data row by row with SCD Type 2\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes_fixed(df, engine)\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        for _, row in df.iterrows():\n",
        "            conn.execute(\n",
        "                text(\"\"\"\n",
        "                    INSERT INTO employees\n",
        "                    VALUES (:employee_id, :name, :email, :address, :phone,\n",
        "                           :date_of_birth, :gender, :company, :position,\n",
        "                           :salary, :retired, :valid_from, :valid_to, :is_current)\n",
        "                \"\"\"),\n",
        "                row.to_dict()\n",
        "            )\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_bulk_pandas(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data using pandas bulk insert\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes_fixed(df, engine)\n",
        "    df.to_sql('employees', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_streaming_chunks(df: pd.DataFrame, engine, chunk_size=1000):\n",
        "    \"\"\"Load data in chunks\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes_fixed(df, engine)  # Use fixed function\n",
        "\n",
        "    for chunk_start in range(0, len(df), chunk_size):\n",
        "        chunk = df.iloc[chunk_start:chunk_start + chunk_size]\n",
        "        chunk.to_sql('employees', engine, if_exists='append', index=False, method='multi')\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "def parallel_worker(chunk_data):\n",
        "    \"\"\"Worker function for parallel processing with proper connection and error handling\"\"\"\n",
        "    try:\n",
        "        # Use the correct database name\n",
        "        engine = create_engine(\"postgresql://postgres:password@localhost:5432/employees\")\n",
        "\n",
        "        # Use with context to ensure proper resource management\n",
        "        with engine.begin() as conn:\n",
        "            # Use if_exists='append' to ensure we don't recreate the table\n",
        "            chunk_data.to_sql('employees', conn, if_exists='append', index=False, method='multi')\n",
        "\n",
        "        return len(chunk_data)  # Return the number of records processed\n",
        "    except Exception as e:\n",
        "        print(f\"Worker error: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Fixed parallel processing method\n",
        "@monitor_performance\n",
        "def load_parallel(df: pd.DataFrame, engine, num_processes=4):\n",
        "    \"\"\"Load data using parallel processing with proper error handling\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        df = identify_changes(df, engine)\n",
        "        df = apply_scd2_changes_fixed(df, engine)  # Use fixed function\n",
        "\n",
        "        # Split the dataframe into chunks\n",
        "        chunks = np.array_split(df, num_processes)\n",
        "        print(f\"Split data into {len(chunks)} chunks of approximately {len(df) // num_processes} records each\")\n",
        "\n",
        "        # Use Pool to process chunks in parallel with proper error handling\n",
        "        with Pool(num_processes) as pool:\n",
        "            try:\n",
        "                # Use map_async with get() to catch worker exceptions\n",
        "                results = pool.map_async(parallel_worker, chunks)\n",
        "                processed_counts = results.get()  # This will raise any exceptions from workers\n",
        "                total_processed = sum(processed_counts) if processed_counts else 0\n",
        "                print(f\"Successfully processed {total_processed} records in parallel\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in parallel processing: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in load_parallel: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_dask(df: pd.DataFrame, engine, npartitions=4):\n",
        "    \"\"\"Load data using Dask\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes_fixed(df, engine)  # Use fixed function\n",
        "\n",
        "    ddf = dd.from_pandas(df, npartitions=npartitions)\n",
        "    with ProgressBar():\n",
        "        for partition in ddf.partitions:\n",
        "            partition.compute().to_sql('employees', engine, if_exists='append', index=False)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_postgres_copy(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data using PostgreSQL COPY command with proper data type handling\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        df = identify_changes(df, engine)\n",
        "        # Use the fixed SCD2 function with reasonable timestamp\n",
        "        df = apply_scd2_changes_fixed(df, engine)\n",
        "\n",
        "        # Create a copy of the dataframe to avoid modifying the original\n",
        "        copy_df = df.copy()\n",
        "\n",
        "        # Convert timestamp columns to proper format\n",
        "        copy_df['valid_from'] = pd.to_datetime(copy_df['valid_from']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        copy_df['valid_to'] = pd.to_datetime(copy_df['valid_to']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Convert date_of_birth to date format\n",
        "        copy_df['date_of_birth'] = pd.to_datetime(copy_df['date_of_birth']).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "        # Handle boolean values\n",
        "        copy_df['is_current'] = copy_df['is_current'].map({True: 't', False: 'f'})\n",
        "\n",
        "        # Escape special characters in text fields\n",
        "        text_cols = ['name', 'email', 'address', 'phone', 'gender', 'company', 'position', 'retired']\n",
        "        for col in text_cols:\n",
        "            copy_df[col] = copy_df[col].astype(str).str.replace('\\t', ' ').str.replace('\\n', ' ')\n",
        "\n",
        "        # Ensure numeric types\n",
        "        copy_df['employee_id'] = copy_df['employee_id'].astype(int)\n",
        "        copy_df['salary'] = copy_df['salary'].astype(float)\n",
        "\n",
        "        # Write to a temporary CSV file with proper formatting\n",
        "        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as f:\n",
        "            copy_df.to_csv(f, index=False, header=False, sep='\\t',\n",
        "                         na_rep='\\\\N', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
        "            temp_file_path = f.name\n",
        "\n",
        "        # Debug: Print column counts and sample data\n",
        "        print(f\"DataFrame has {len(copy_df.columns)} columns: {', '.join(copy_df.columns)}\")\n",
        "        if len(copy_df) > 0:\n",
        "            print(f\"Sample row (first 3 values): {list(copy_df.iloc[0].values)[:3]}\")\n",
        "\n",
        "        try:\n",
        "            # Get a raw connection directly - don't use it as a context manager\n",
        "            raw_conn = engine.raw_connection()\n",
        "            cursor = raw_conn.cursor()\n",
        "\n",
        "            try:\n",
        "                with open(temp_file_path, 'r', encoding='utf-8') as f:\n",
        "                    print(f\"Executing COPY with columns: {', '.join(copy_df.columns)}\")\n",
        "                    cursor.copy_from(\n",
        "                        f,\n",
        "                        'employees',\n",
        "                        sep='\\t',\n",
        "                        columns=copy_df.columns.tolist(),\n",
        "                        null='\\\\N'\n",
        "                    )\n",
        "                # Commit the transaction\n",
        "                raw_conn.commit()\n",
        "                print(f\"COPY operation successful, loaded {len(copy_df)} records\")\n",
        "            except Exception as e:\n",
        "                # Rollback on error\n",
        "                raw_conn.rollback()\n",
        "                print(f\"COPY operation failed: {str(e)}\")\n",
        "                # If we get an error, try reading the first few lines of the file for debugging\n",
        "                with open(temp_file_path, 'r', encoding='utf-8') as debug_f:\n",
        "                    first_lines = [next(debug_f) for _ in range(min(3, len(copy_df)))]\n",
        "                    print(f\"First few lines of CSV:\\n{''.join(first_lines)}\")\n",
        "                raise\n",
        "            finally:\n",
        "                # Close the cursor and connection\n",
        "                cursor.close()\n",
        "                raw_conn.close()\n",
        "        finally:\n",
        "            os.remove(temp_file_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in PostgreSQL COPY: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1. Setup Database\n",
        "    print(\"\\n=== 1. Setting up PostgreSQL Database ===\")\n",
        "    connection_string = setup_postgres_colab()\n",
        "    engine = create_engine(connection_string)\n",
        "    setup_database(engine)\n",
        "\n",
        "    # Define test data sizes\n",
        "    data_sizes = [\n",
        "        (1000000, 100000),\n",
        "        (100000, 10000),\n",
        "        (10000, 1000),\n",
        "        (1000, 100),\n",
        "        (100, 10)\n",
        "    ]\n",
        "\n",
        "    # Define the loading methods\n",
        "    methods = [\n",
        "        (load_postgres_copy, \"PostgreSQL COPY\"),\n",
        "        (load_parallel, \"Parallel Processing\"),\n",
        "        (load_row_by_row, \"Row-by-row\"),\n",
        "        (load_bulk_pandas, \"Bulk Pandas\"),\n",
        "        (load_streaming_chunks, \"Streaming Chunks\"),\n",
        "        (load_dask, \"Dask\")\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # Loop through different data sizes\n",
        "    for initial_size, update_size in data_sizes:\n",
        "        print(f\"\\n\\n================================================\")\n",
        "        print(f\"Testing with Initial Size: {initial_size}, Update Size: {update_size}\")\n",
        "        print(f\"================================================\")\n",
        "\n",
        "        # Reset the table at the beginning of each data size test to ensure we start fresh\n",
        "        print(\"\\nResetting table for new data size test...\")\n",
        "        reset_table(engine)\n",
        "\n",
        "        # 2. Create Initial Load File\n",
        "        print(f\"\\n=== 2. Creating Initial Load File ({initial_size} records) ===\")\n",
        "        initial_df = generate_data(initial_size)\n",
        "        print(f\"Generated {len(initial_df)} records for initial load\")\n",
        "\n",
        "        # Print a sample record to validate data structure\n",
        "        print(\"\\nSample record:\")\n",
        "        if len(initial_df) > 0:\n",
        "            sample = initial_df.iloc[0].to_dict()\n",
        "            for k, v in sample.items():\n",
        "                print(f\"{k}: {v} ({type(v).__name__})\")\n",
        "\n",
        "        # 3. Create Subsequent Load File\n",
        "        print(f\"\\n=== 3. Creating Subsequent Load File ({update_size} records) ===\")\n",
        "        update_df = generate_data(update_size)\n",
        "        print(f\"Generated {len(update_df)} records for subsequent load\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Execute methods sequentially\n",
        "        for idx, (method, name) in enumerate(methods, start=1):\n",
        "            print(f\"\\n=== Method {idx}: {name} ===\")\n",
        "\n",
        "            # Reset table before each method (except the first one since we just reset it)\n",
        "            if idx > 1:\n",
        "                print(f\"\\nResetting table for method {name}...\")\n",
        "                reset_table(engine)\n",
        "\n",
        "            try:\n",
        "                # 4. Load Initial File\n",
        "                print(f\"\\nLoading initial file...\")\n",
        "                initial_duration, initial_stats = method(initial_df, engine)\n",
        "\n",
        "                # Get count after initial load with error handling\n",
        "                try:\n",
        "                    with engine.connect() as conn:\n",
        "                        initial_count = conn.execute(text(\"SELECT COUNT(*) FROM employees\")).scalar()\n",
        "                        print(f\"Count after initial load: {initial_count} records\")\n",
        "\n",
        "                        # Print a sample from the database to verify data was inserted correctly\n",
        "                        if initial_count > 0:\n",
        "                            sample = conn.execute(text(\"SELECT * FROM employees LIMIT 1\")).fetchone()\n",
        "                            print(f\"Sample DB record: {sample}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error checking initial count: {str(e)}\")\n",
        "                    initial_count = 0\n",
        "\n",
        "                # 5. Load Subsequent File\n",
        "                print(f\"\\nLoading subsequent file...\")\n",
        "                update_duration, update_stats = method(update_df, engine)\n",
        "\n",
        "                # Get final count with error handling\n",
        "                try:\n",
        "                    with engine.connect() as conn:\n",
        "                        final_count = conn.execute(text(\"SELECT COUNT(*) FROM employees\")).scalar()\n",
        "                        print(f\"Final count: {final_count} records\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error checking final count: {str(e)}\")\n",
        "                    final_count = initial_count  # Assume no change if we can't check\n",
        "\n",
        "                result = {\n",
        "                    'Method': name,\n",
        "                    'Initial Size': initial_size,\n",
        "                    'Update Size': update_size,\n",
        "                    'Initial Load Time': f\"{initial_duration:.2f}s\",\n",
        "                    'Initial Records': initial_count,\n",
        "                    'Initial Avg CPU': f\"{initial_stats['avg_cpu']:.1f}%\",\n",
        "                    'Initial Max CPU': f\"{initial_stats['max_cpu']:.1f}%\",\n",
        "                    'Initial Avg Memory': f\"{initial_stats['avg_memory']:.1f}%\",\n",
        "                    'Initial Max Memory': f\"{initial_stats['max_memory']:.1f}%\",\n",
        "                    'Update Load Time': f\"{update_duration:.2f}s\",\n",
        "                    'Update Records': final_count - initial_count,\n",
        "                    'Update Avg CPU': f\"{update_stats['avg_cpu']:.1f}%\",\n",
        "                    'Update Max CPU': f\"{update_stats['max_cpu']:.1f}%\",\n",
        "                    'Update Avg Memory': f\"{update_stats['avg_memory']:.1f}%\",\n",
        "                    'Update Max Memory': f\"{update_stats['max_memory']:.1f}%\",\n",
        "                    'Final Records': final_count,\n",
        "                    'Total Time': f\"{(initial_duration + update_duration):.2f}s\"\n",
        "                }\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "                print(f\"\\nMethod {idx} Results:\")\n",
        "                print(f\"Initial Load: {initial_duration:.2f}s ({initial_count} records)\")\n",
        "                print(f\"Initial Load Resource Usage:\")\n",
        "                print(f\"  Avg CPU: {initial_stats['avg_cpu']:.1f}%, Max CPU: {initial_stats['max_cpu']:.1f}%\")\n",
        "                print(f\"  Avg Memory: {initial_stats['avg_memory']:.1f}%, Max Memory: {initial_stats['max_memory']:.1f}%\")\n",
        "                print(f\"\\nUpdate Load: {update_duration:.2f}s ({final_count - initial_count} records)\")\n",
        "                print(f\"Update Load Resource Usage:\")\n",
        "                print(f\"  Avg CPU: {update_stats['avg_cpu']:.1f}%, Max CPU: {update_stats['max_cpu']:.1f}%\")\n",
        "                print(f\"  Avg Memory: {update_stats['avg_memory']:.1f}%, Max Memory: {update_stats['max_memory']:.1f}%\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in {name} method: {str(e)}\")\n",
        "                results.append({\n",
        "                    'Method': name,\n",
        "                    'Initial Size': initial_size,\n",
        "                    'Update Size': update_size,\n",
        "                    'Initial Load Time': 'Failed',\n",
        "                    'Initial Records': 'Failed',\n",
        "                    'Initial Avg CPU': 'Failed',\n",
        "                    'Initial Max CPU': 'Failed',\n",
        "                    'Initial Avg Memory': 'Failed',\n",
        "                    'Initial Max Memory': 'Failed',\n",
        "                    'Update Load Time': 'Failed',\n",
        "                    'Update Records': 'Failed',\n",
        "                    'Update Avg CPU': 'Failed',\n",
        "                    'Update Max CPU': 'Failed',\n",
        "                    'Update Avg Memory': 'Failed',\n",
        "                    'Update Max Memory': 'Failed',\n",
        "                    'Final Records': 'Failed',\n",
        "                    'Total Time': 'Failed'\n",
        "                })\n",
        "\n",
        "        all_results[f\"{initial_size}_{update_size}\"] = results\n",
        "\n",
        "        # Print results for current data size\n",
        "        print(f\"\\nResults for Initial Size: {initial_size}, Update Size: {update_size}\")\n",
        "        print(\"=\" * 140)\n",
        "        headers = [\n",
        "            'Method', 'Initial Load Time', 'Initial Records', 'Initial Avg CPU', 'Initial Max Memory',\n",
        "            'Update Load Time', 'Update Records', 'Update Avg CPU', 'Update Max Memory',\n",
        "            'Total Time'\n",
        "        ]\n",
        "        row_format = \"{:<20} {:<20} {:<15} {:<15} {:<20} {:<20} {:<15} {:<15} {:<20} {:<15}\"\n",
        "        print(row_format.format(*headers))\n",
        "        print(\"-\" * 140)\n",
        "        for result in results:\n",
        "            print(row_format.format(\n",
        "                result['Method'],\n",
        "                result['Initial Load Time'],\n",
        "                str(result['Initial Records']),\n",
        "                result['Initial Avg CPU'],\n",
        "                result['Initial Max Memory'],\n",
        "                result['Update Load Time'],\n",
        "                str(result['Update Records']),\n",
        "                result['Update Avg CPU'],\n",
        "                result['Update Max Memory'],\n",
        "                result['Total Time']\n",
        "            ))\n",
        "\n",
        "    # Print comparative summary across all data sizes\n",
        "    print(\"\\n\\nComparative Summary Across All Data Sizes\")\n",
        "    print(\"=\" * 100)\n",
        "    print(\"Data Size (Initial, Update) | Best Method | Worst Method | Average Load Time\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for initial_size, update_size in data_sizes:\n",
        "        results = all_results[f\"{initial_size}_{update_size}\"]\n",
        "        valid_results = [r for r in results if r['Total Time'] != 'Failed']\n",
        "\n",
        "        if valid_results:\n",
        "            # Convert time strings to float (removing 's' suffix)\n",
        "            for r in valid_results:\n",
        "                if isinstance(r['Total Time'], str) and r['Total Time'].endswith('s'):\n",
        "                    r['Total Time'] = float(r['Total Time'][:-1])\n",
        "\n",
        "            best_method = min(valid_results, key=lambda x: x['Total Time'])\n",
        "            worst_method = max(valid_results, key=lambda x: x['Total Time'])\n",
        "            avg_time = sum(r['Total Time'] for r in valid_results) / len(valid_results)\n",
        "\n",
        "            print(f\"{initial_size:,}, {update_size:,} | {best_method['Method']} ({best_method['Total Time']:.2f}s) | \"\n",
        "                  f\"{worst_method['Method']} ({worst_method['Total Time']:.2f}s) | {avg_time:.2f}s\")\n",
        "        else:\n",
        "            print(f\"{initial_size:,}, {update_size:,} | All methods failed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_0tIFbdSxnR",
        "outputId": "05806056-2b8f-44f1-8862-9df3d4424976"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 1. Setting up PostgreSQL Database ===\n",
            "Setting up PostgreSQL in Google Colab...\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "postgresql is already the newest version (14+238).\n",
            "postgresql-contrib is already the newest version (14+238).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "ALTER ROLE\n",
            "CREATE DATABASE\n",
            "host all all 0.0.0.0/0 md5\n",
            "listen_addresses = '*'\n",
            " * Restarting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "Attempting to connect to database... (Attempt 1/30)\n",
            "Successfully connected to PostgreSQL!\n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 1000000, Update Size: 100000\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (1000000 records) ===\n",
            "Generated 1000000 records for initial load\n",
            "\n",
            "Sample record:\n",
            "employee_id: 0 (int)\n",
            "name: Danilo Acevedo (str)\n",
            "email: cursor2052@protonmail.com (str)\n",
            "address: 591 Hillway Glen (str)\n",
            "phone: +19561599585 (str)\n",
            "date_of_birth: 1973-06-02 (str)\n",
            "gender: Male (str)\n",
            "company: Poplar Bluff Corp (str)\n",
            "position: Meat Inspector (str)\n",
            "salary: 36535.48 (float)\n",
            "retired: No (str)\n",
            "\n",
            "=== 3. Creating Subsequent Load File (100000 records) ===\n",
            "Generated 100000 records for subsequent load\n",
            "\n",
            "=== Method 1: PostgreSQL COPY ===\n",
            "\n",
            "Loading initial file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Danilo Acevedo', 'cursor2052@protonmail.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 1000000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.59%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.88%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000000 records\n",
            "Sample DB record: (0, 'Danilo Acevedo', 'cursor2052@protonmail.com', '591 Hillway Glen', '+19561599585', datetime.date(1973, 6, 2), 'Male', 'Poplar Bluff Corp', 'Meat Inspector', Decimal('36535.48'), 'No', datetime.datetime(2025, 2, 26, 23, 26, 11), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Maxwell Guerrero', 'nails1834@outlook.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 100000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.60%\n",
            "Max CPU Usage: 2.50%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 1.94%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Method 1 Results:\n",
            "Initial Load: 20.42s (1000000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.6%, Max CPU: 2.9%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 13.06s (100000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.6%, Max CPU: 2.5%\n",
            "  Avg Memory: 1.9%, Max Memory: 2.1%\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Resetting table for method Parallel Processing...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into 4 chunks of approximately 250000 records each\n",
            "Successfully processed 1000000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 4.72%\n",
            "Max CPU Usage: 6.20%\n",
            "Min CPU Usage: 1.50%\n",
            "Average Memory Usage: 4.42%\n",
            "Max Memory Usage: 8.50%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000000 records\n",
            "Sample DB record: (0, 'Danilo Acevedo', 'cursor2052@protonmail.com', '591 Hillway Glen', '+19561599585', datetime.date(1973, 6, 2), 'Male', 'Poplar Bluff Corp', 'Meat Inspector', Decimal('36535.48'), 'No', datetime.datetime(2025, 2, 26, 23, 26, 45, 973772), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 25000 records each\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 3.22%\n",
            "Max CPU Usage: 6.40%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 2.07%\n",
            "Max Memory Usage: 2.60%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Method 2 Results:\n",
            "Initial Load: 80.57s (1000000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 4.7%, Max CPU: 6.2%\n",
            "  Avg Memory: 4.4%, Max Memory: 8.5%\n",
            "\n",
            "Update Load: 20.00s (100000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 3.2%, Max CPU: 6.4%\n",
            "  Avg Memory: 2.1%, Max Memory: 2.6%\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Resetting table for method Row-by-row...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.54%\n",
            "Max CPU Usage: 3.00%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000000 records\n",
            "Sample DB record: (0, 'Danilo Acevedo', 'cursor2052@protonmail.com', '591 Hillway Glen', '+19561599585', datetime.date(1973, 6, 2), 'Male', 'Poplar Bluff Corp', 'Meat Inspector', Decimal('36535.48'), 'No', datetime.datetime(2025, 2, 26, 23, 28, 26, 988654), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.49%\n",
            "Max CPU Usage: 3.30%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 1.86%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Method 3 Results:\n",
            "Initial Load: 475.42s (1000000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 3.0%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 58.51s (100000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 3.3%\n",
            "  Avg Memory: 1.9%, Max Memory: 2.1%\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Resetting table for method Bulk Pandas...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.51%\n",
            "Max CPU Usage: 3.10%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "Count after initial load: 1000000 records\n",
            "Sample DB record: (0, 'Danilo Acevedo', 'cursor2052@protonmail.com', '591 Hillway Glen', '+19561599585', datetime.date(1973, 6, 2), 'Male', 'Poplar Bluff Corp', 'Meat Inspector', Decimal('36535.48'), 'No', datetime.datetime(2025, 2, 26, 23, 37, 21, 756813), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.49%\n",
            "Max CPU Usage: 3.40%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 1.93%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 1.90%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Method 4 Results:\n",
            "Initial Load: 221.12s (1000000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 3.1%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 33.58s (100000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 3.4%\n",
            "  Avg Memory: 1.9%, Max Memory: 2.1%\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Resetting table for method Streaming Chunks...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.50%\n",
            "Max CPU Usage: 3.00%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "Count after initial load: 1000000 records\n",
            "Sample DB record: (0, 'Danilo Acevedo', 'cursor2052@protonmail.com', '591 Hillway Glen', '+19561599585', datetime.date(1973, 6, 2), 'Male', 'Poplar Bluff Corp', 'Meat Inspector', Decimal('36535.48'), 'No', datetime.datetime(2025, 2, 26, 23, 41, 38, 147130), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.48%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 1.93%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 1.90%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Method 5 Results:\n",
            "Initial Load: 227.64s (1000000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 3.0%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 35.01s (100000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 2.9%\n",
            "  Avg Memory: 1.9%, Max Memory: 2.1%\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Resetting table for method Dask...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.95 ms\n",
            "[########################################] | 100% Completed | 101.70 ms\n",
            "[########################################] | 100% Completed | 101.98 ms\n",
            "[########################################] | 100% Completed | 101.77 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.49%\n",
            "Max CPU Usage: 3.10%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 1.90%\n",
            "Count after initial load: 1000000 records\n",
            "Sample DB record: (0, 'Danilo Acevedo', 'cursor2052@protonmail.com', '591 Hillway Glen', '+19561599585', datetime.date(1973, 6, 2), 'Male', 'Poplar Bluff Corp', 'Meat Inspector', Decimal('36535.48'), 'No', datetime.datetime(2025, 2, 26, 23, 46, 2, 257867), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.89 ms\n",
            "[########################################] | 100% Completed | 101.69 ms\n",
            "[########################################] | 100% Completed | 102.02 ms\n",
            "[########################################] | 100% Completed | 101.86 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.49%\n",
            "Max CPU Usage: 2.80%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.06%\n",
            "Max Memory Usage: 2.20%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Method 6 Results:\n",
            "Initial Load: 64.32s (1000000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 3.1%\n",
            "  Avg Memory: 2.0%, Max Memory: 2.0%\n",
            "\n",
            "Update Load: 18.36s (100000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 2.8%\n",
            "  Avg Memory: 2.1%, Max Memory: 2.2%\n",
            "\n",
            "Results for Initial Size: 1000000, Update Size: 100000\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PostgreSQL COPY      20.42s               1000000         1.6%            1.9%                 13.06s               100000          1.6%            2.1%                 33.48s         \n",
            "Parallel Processing  80.57s               1000000         4.7%            8.5%                 20.00s               100000          3.2%            2.6%                 100.58s        \n",
            "Row-by-row           475.42s              1000000         1.5%            1.9%                 58.51s               100000          1.5%            2.1%                 533.94s        \n",
            "Bulk Pandas          221.12s              1000000         1.5%            1.9%                 33.58s               100000          1.5%            2.1%                 254.70s        \n",
            "Streaming Chunks     227.64s              1000000         1.5%            1.9%                 35.01s               100000          1.5%            2.1%                 262.65s        \n",
            "Dask                 64.32s               1000000         1.5%            2.0%                 18.36s               100000          1.5%            2.2%                 82.68s         \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 100000, Update Size: 10000\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (100000 records) ===\n",
            "Generated 100000 records for initial load\n",
            "\n",
            "Sample record:\n",
            "employee_id: 0 (int)\n",
            "name: Moses Joyce (str)\n",
            "email: eight1819@duck.com (str)\n",
            "address: 578 Edna Bayou (str)\n",
            "phone: +1-620-328-8585 (str)\n",
            "date_of_birth: 1976-01-31 (str)\n",
            "gender: Female (str)\n",
            "company: Hudson Corp (str)\n",
            "position: Publicity Manager (str)\n",
            "salary: 130909.12 (float)\n",
            "retired: Yes (str)\n",
            "\n",
            "=== 3. Creating Subsequent Load File (10000 records) ===\n",
            "Generated 10000 records for subsequent load\n",
            "\n",
            "=== Method 1: PostgreSQL COPY ===\n",
            "\n",
            "Loading initial file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Moses Joyce', 'eight1819@duck.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 100000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.60%\n",
            "Max CPU Usage: 2.10%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100000 records\n",
            "Sample DB record: (0, 'Moses Joyce', 'eight1819@duck.com', '578 Edna Bayou', '+1-620-328-8585', datetime.date(1976, 1, 31), 'Female', 'Hudson Corp', 'Publicity Manager', Decimal('130909.12'), 'Yes', datetime.datetime(2025, 2, 26, 23, 47, 30), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Federico Farrell', 'occasionally1892@live.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 10000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 2.35%\n",
            "Max CPU Usage: 3.00%\n",
            "Min CPU Usage: 1.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110000 records\n",
            "\n",
            "Method 1 Results:\n",
            "Initial Load: 1.95s (100000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.6%, Max CPU: 2.1%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 1.23s (10000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 2.4%, Max CPU: 3.0%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Resetting table for method Parallel Processing...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 25000 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 3.84%\n",
            "Max CPU Usage: 4.40%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 2.14%\n",
            "Max Memory Usage: 2.60%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100000 records\n",
            "Sample DB record: (25000, 'Grayce Mejia', 'waste1830@live.com', '1246 Ross Bayou', '+1-364-435-3140', datetime.date(1984, 6, 20), 'Female', 'Pinellas Park Corp', 'Sportswoman', Decimal('152849.36'), 'No', datetime.datetime(2025, 2, 26, 23, 47, 34, 167581), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 2500 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 10000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 3.20%\n",
            "Max CPU Usage: 4.10%\n",
            "Min CPU Usage: 2.70%\n",
            "Average Memory Usage: 1.87%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110000 records\n",
            "\n",
            "Method 2 Results:\n",
            "Initial Load: 8.40s (100000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 3.8%, Max CPU: 4.4%\n",
            "  Avg Memory: 2.1%, Max Memory: 2.6%\n",
            "\n",
            "Update Load: 2.05s (10000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 3.2%, Max CPU: 4.1%\n",
            "  Avg Memory: 1.9%, Max Memory: 2.0%\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Resetting table for method Row-by-row...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.43%\n",
            "Max CPU Usage: 3.10%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100000 records\n",
            "Sample DB record: (0, 'Moses Joyce', 'eight1819@duck.com', '578 Edna Bayou', '+1-620-328-8585', datetime.date(1976, 1, 31), 'Female', 'Hudson Corp', 'Publicity Manager', Decimal('130909.12'), 'Yes', datetime.datetime(2025, 2, 26, 23, 47, 46, 236024), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.10%\n",
            "Max CPU Usage: 1.30%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110000 records\n",
            "\n",
            "Method 3 Results:\n",
            "Initial Load: 46.53s (100000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.4%, Max CPU: 3.1%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 5.66s (10000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.1%, Max CPU: 1.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Resetting table for method Bulk Pandas...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.50%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100000 records\n",
            "Sample DB record: (0, 'Moses Joyce', 'eight1819@duck.com', '578 Edna Bayou', '+1-620-328-8585', datetime.date(1976, 1, 31), 'Female', 'Hudson Corp', 'Publicity Manager', Decimal('130909.12'), 'Yes', datetime.datetime(2025, 2, 26, 23, 48, 39, 365556), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.72%\n",
            "Max CPU Usage: 2.80%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110000 records\n",
            "\n",
            "Method 4 Results:\n",
            "Initial Load: 22.23s (100000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 2.9%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 3.28s (10000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.7%, Max CPU: 2.8%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Resetting table for method Streaming Chunks...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.45%\n",
            "Max CPU Usage: 2.70%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100000 records\n",
            "Sample DB record: (0, 'Moses Joyce', 'eight1819@duck.com', '578 Edna Bayou', '+1-620-328-8585', datetime.date(1976, 1, 31), 'Female', 'Hudson Corp', 'Publicity Manager', Decimal('130909.12'), 'Yes', datetime.datetime(2025, 2, 26, 23, 49, 6, 634983), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.18%\n",
            "Max CPU Usage: 1.30%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110000 records\n",
            "\n",
            "Method 5 Results:\n",
            "Initial Load: 23.25s (100000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 2.7%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 3.33s (10000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.2%, Max CPU: 1.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Resetting table for method Dask...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.66 ms\n",
            "[########################################] | 100% Completed | 101.63 ms\n",
            "[########################################] | 100% Completed | 101.66 ms\n",
            "[########################################] | 100% Completed | 101.57 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.46%\n",
            "Max CPU Usage: 3.00%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100000 records\n",
            "Sample DB record: (0, 'Moses Joyce', 'eight1819@duck.com', '578 Edna Bayou', '+1-620-328-8585', datetime.date(1976, 1, 31), 'Female', 'Hudson Corp', 'Publicity Manager', Decimal('130909.12'), 'Yes', datetime.datetime(2025, 2, 26, 23, 49, 34, 641034), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.62 ms\n",
            "[########################################] | 100% Completed | 101.58 ms\n",
            "[########################################] | 100% Completed | 101.73 ms\n",
            "[########################################] | 100% Completed | 101.65 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.87%\n",
            "Max CPU Usage: 1.30%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110000 records\n",
            "\n",
            "Method 6 Results:\n",
            "Initial Load: 7.22s (100000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.5%, Max CPU: 3.0%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 2.11s (10000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.9%, Max CPU: 1.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Results for Initial Size: 100000, Update Size: 10000\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PostgreSQL COPY      1.95s                100000          1.6%            1.8%                 1.23s                10000           2.4%            1.8%                 3.17s          \n",
            "Parallel Processing  8.40s                100000          3.8%            2.6%                 2.05s                10000           3.2%            2.0%                 10.45s         \n",
            "Row-by-row           46.53s               100000          1.4%            1.8%                 5.66s                10000           1.1%            1.8%                 52.19s         \n",
            "Bulk Pandas          22.23s               100000          1.5%            1.8%                 3.28s                10000           1.7%            1.8%                 25.52s         \n",
            "Streaming Chunks     23.25s               100000          1.5%            1.8%                 3.33s                10000           1.2%            1.8%                 26.58s         \n",
            "Dask                 7.22s                100000          1.5%            1.8%                 2.11s                10000           0.9%            1.8%                 9.34s          \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 10000, Update Size: 1000\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (10000 records) ===\n",
            "Generated 10000 records for initial load\n",
            "\n",
            "Sample record:\n",
            "employee_id: 0 (int)\n",
            "name: Britta Hardin (str)\n",
            "email: glory1888@duck.com (str)\n",
            "address: 888 Mcnair Walk (str)\n",
            "phone: +16088813132 (str)\n",
            "date_of_birth: 1999-05-17 (str)\n",
            "gender: Female (str)\n",
            "company: Barnstable Town Corp (str)\n",
            "position: Instrument Engineer (str)\n",
            "salary: 49397.57 (float)\n",
            "retired: No (str)\n",
            "\n",
            "=== 3. Creating Subsequent Load File (1000 records) ===\n",
            "Generated 1000 records for subsequent load\n",
            "\n",
            "=== Method 1: PostgreSQL COPY ===\n",
            "\n",
            "Loading initial file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Britta Hardin', 'glory1888@duck.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 10000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 10000 records\n",
            "Sample DB record: (0, 'Britta Hardin', 'glory1888@duck.com', '888 Mcnair Walk', '+16088813132', datetime.date(1999, 5, 17), 'Female', 'Barnstable Town Corp', 'Instrument Engineer', Decimal('49397.57'), 'No', datetime.datetime(2025, 2, 26, 23, 49, 46), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Wendie Duncan', 'fur1839@example.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 1000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 11000 records\n",
            "\n",
            "Method 1 Results:\n",
            "Initial Load: 0.21s (10000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.5%, Max CPU: 0.5%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.13s (1000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.4%, Max CPU: 0.4%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Resetting table for method Parallel Processing...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 2500 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 10000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 10000 records\n",
            "Sample DB record: (0, 'Britta Hardin', 'glory1888@duck.com', '888 Mcnair Walk', '+16088813132', datetime.date(1999, 5, 17), 'Female', 'Barnstable Town Corp', 'Instrument Engineer', Decimal('49397.57'), 'No', datetime.datetime(2025, 2, 26, 23, 49, 48, 290797), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 250 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 1000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 3.70%\n",
            "Max CPU Usage: 3.70%\n",
            "Min CPU Usage: 3.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 11000 records\n",
            "\n",
            "Method 2 Results:\n",
            "Initial Load: 0.96s (10000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.3%, Max CPU: 0.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.33s (1000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 3.7%, Max CPU: 3.7%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Resetting table for method Row-by-row...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.84%\n",
            "Max CPU Usage: 3.10%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 10000 records\n",
            "Sample DB record: (0, 'Britta Hardin', 'glory1888@duck.com', '888 Mcnair Walk', '+16088813132', datetime.date(1999, 5, 17), 'Female', 'Barnstable Town Corp', 'Instrument Engineer', Decimal('49397.57'), 'No', datetime.datetime(2025, 2, 26, 23, 49, 50, 321316), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.90%\n",
            "Max CPU Usage: 0.90%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 11000 records\n",
            "\n",
            "Method 3 Results:\n",
            "Initial Load: 4.76s (10000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.8%, Max CPU: 3.1%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.55s (1000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.9%, Max CPU: 0.9%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Resetting table for method Bulk Pandas...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.03%\n",
            "Max CPU Usage: 1.20%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 10000 records\n",
            "Sample DB record: (0, 'Britta Hardin', 'glory1888@duck.com', '888 Mcnair Walk', '+16088813132', datetime.date(1999, 5, 17), 'Female', 'Barnstable Town Corp', 'Instrument Engineer', Decimal('49397.57'), 'No', datetime.datetime(2025, 2, 26, 23, 49, 56, 358095), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 11000 records\n",
            "\n",
            "Method 4 Results:\n",
            "Initial Load: 2.38s (10000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.0%, Max CPU: 1.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.44s (1000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.5%, Max CPU: 0.5%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Resetting table for method Streaming Chunks...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.17%\n",
            "Max CPU Usage: 1.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 10000 records\n",
            "Sample DB record: (0, 'Britta Hardin', 'glory1888@duck.com', '888 Mcnair Walk', '+16088813132', datetime.date(1999, 5, 17), 'Female', 'Barnstable Town Corp', 'Instrument Engineer', Decimal('49397.57'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 0, 471598), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.80%\n",
            "Max CPU Usage: 1.80%\n",
            "Min CPU Usage: 1.80%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 11000 records\n",
            "\n",
            "Method 5 Results:\n",
            "Initial Load: 2.51s (10000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.2%, Max CPU: 1.6%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.47s (1000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.8%, Max CPU: 1.8%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Resetting table for method Dask...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.96 ms\n",
            "[########################################] | 100% Completed | 101.76 ms\n",
            "[########################################] | 100% Completed | 102.07 ms\n",
            "[########################################] | 100% Completed | 101.84 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.75%\n",
            "Max CPU Usage: 2.20%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 10000 records\n",
            "Sample DB record: (0, 'Britta Hardin', 'glory1888@duck.com', '888 Mcnair Walk', '+16088813132', datetime.date(1999, 5, 17), 'Female', 'Barnstable Town Corp', 'Instrument Engineer', Decimal('49397.57'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 4, 563900), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 102.13 ms\n",
            "[########################################] | 100% Completed | 101.38 ms\n",
            "[########################################] | 100% Completed | 101.53 ms\n",
            "[########################################] | 100% Completed | 101.51 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 11000 records\n",
            "\n",
            "Method 6 Results:\n",
            "Initial Load: 1.16s (10000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.8%, Max CPU: 2.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.64s (1000 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.6%, Max CPU: 0.6%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Results for Initial Size: 10000, Update Size: 1000\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PostgreSQL COPY      0.21s                10000           0.5%            1.8%                 0.13s                1000            0.4%            1.8%                 0.34s          \n",
            "Parallel Processing  0.96s                10000           0.3%            1.8%                 0.33s                1000            3.7%            1.8%                 1.29s          \n",
            "Row-by-row           4.76s                10000           1.8%            1.8%                 0.55s                1000            0.9%            1.8%                 5.32s          \n",
            "Bulk Pandas          2.38s                10000           1.0%            1.8%                 0.44s                1000            0.5%            1.8%                 2.81s          \n",
            "Streaming Chunks     2.51s                10000           1.2%            1.8%                 0.47s                1000            1.8%            1.8%                 2.98s          \n",
            "Dask                 1.16s                10000           1.8%            1.8%                 0.64s                1000            0.6%            1.8%                 1.80s          \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 1000, Update Size: 100\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (1000 records) ===\n",
            "Generated 1000 records for initial load\n",
            "\n",
            "Sample record:\n",
            "employee_id: 0 (int)\n",
            "name: Amal Conrad (str)\n",
            "email: functionality1892@yahoo.com (str)\n",
            "address: 268 Cowles Trace (str)\n",
            "phone: +1-205-316-9428 (str)\n",
            "date_of_birth: 1986-08-19 (str)\n",
            "gender: Female (str)\n",
            "company: Lemoore Corp (str)\n",
            "position: Receptionist (str)\n",
            "salary: 150706.05 (float)\n",
            "retired: No (str)\n",
            "\n",
            "=== 3. Creating Subsequent Load File (100 records) ===\n",
            "Generated 100 records for subsequent load\n",
            "\n",
            "=== Method 1: PostgreSQL COPY ===\n",
            "\n",
            "Loading initial file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Amal Conrad', 'functionality1892@yahoo.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 1000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000 records\n",
            "Sample DB record: (0, 'Amal Conrad', 'functionality1892@yahoo.com', '268 Cowles Trace', '+1-205-316-9428', datetime.date(1986, 8, 19), 'Female', 'Lemoore Corp', 'Receptionist', Decimal('150706.05'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 7), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'German Hansen', 'belief2067@duck.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 100 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100 records\n",
            "\n",
            "Method 1 Results:\n",
            "Initial Load: 0.04s (1000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.4%, Max CPU: 0.4%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.04s (100 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Resetting table for method Parallel Processing...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 250 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 1000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000 records\n",
            "Sample DB record: (250, 'Tamekia Beck', 'examines2014@yahoo.com', '1340 Coventry Walk', '+13211564750', datetime.date(1994, 9, 29), 'Male', 'Easley Corp', 'Cab Driver', Decimal('33088.90'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 9, 688816), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 25 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.80%\n",
            "Max CPU Usage: 0.80%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100 records\n",
            "\n",
            "Method 2 Results:\n",
            "Initial Load: 0.26s (1000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.3%, Max CPU: 0.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.21s (100 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.8%, Max CPU: 0.8%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Resetting table for method Row-by-row...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.70%\n",
            "Max CPU Usage: 0.70%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000 records\n",
            "Sample DB record: (0, 'Amal Conrad', 'functionality1892@yahoo.com', '268 Cowles Trace', '+1-205-316-9428', datetime.date(1986, 8, 19), 'Female', 'Lemoore Corp', 'Receptionist', Decimal('150706.05'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 11, 719089), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100 records\n",
            "\n",
            "Method 3 Results:\n",
            "Initial Load: 0.47s (1000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.7%, Max CPU: 0.7%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.07s (100 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.6%, Max CPU: 0.6%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Resetting table for method Bulk Pandas...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000 records\n",
            "Sample DB record: (0, 'Amal Conrad', 'functionality1892@yahoo.com', '268 Cowles Trace', '+1-205-316-9428', datetime.date(1986, 8, 19), 'Female', 'Lemoore Corp', 'Receptionist', Decimal('150706.05'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 13, 744254), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.00%\n",
            "Max CPU Usage: 1.00%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100 records\n",
            "\n",
            "Method 4 Results:\n",
            "Initial Load: 0.21s (1000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.05s (100 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.0%, Max CPU: 1.0%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Resetting table for method Streaming Chunks...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 2.10%\n",
            "Max CPU Usage: 2.10%\n",
            "Min CPU Usage: 2.10%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000 records\n",
            "Sample DB record: (0, 'Amal Conrad', 'functionality1892@yahoo.com', '268 Cowles Trace', '+1-205-316-9428', datetime.date(1986, 8, 19), 'Female', 'Lemoore Corp', 'Receptionist', Decimal('150706.05'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 15, 774098), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.20%\n",
            "Max CPU Usage: 1.20%\n",
            "Min CPU Usage: 1.20%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100 records\n",
            "\n",
            "Method 5 Results:\n",
            "Initial Load: 0.37s (1000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 2.1%, Max CPU: 2.1%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.05s (100 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.2%, Max CPU: 1.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Resetting table for method Dask...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.68 ms\n",
            "[########################################] | 100% Completed | 101.35 ms\n",
            "[########################################] | 100% Completed | 101.58 ms\n",
            "[########################################] | 100% Completed | 101.67 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 1000 records\n",
            "Sample DB record: (0, 'Amal Conrad', 'functionality1892@yahoo.com', '268 Cowles Trace', '+1-205-316-9428', datetime.date(1986, 8, 19), 'Female', 'Lemoore Corp', 'Receptionist', Decimal('150706.05'), 'No', datetime.datetime(2025, 2, 26, 23, 50, 17, 800620), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.47 ms\n",
            "[########################################] | 100% Completed | 101.46 ms\n",
            "[########################################] | 100% Completed | 101.41 ms\n",
            "[########################################] | 100% Completed | 101.46 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 1100 records\n",
            "\n",
            "Method 6 Results:\n",
            "Initial Load: 0.54s (1000 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.6%, Max CPU: 0.6%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.50s (100 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.3%, Max CPU: 0.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Results for Initial Size: 1000, Update Size: 100\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PostgreSQL COPY      0.04s                1000            0.4%            1.8%                 0.04s                100             0.2%            1.8%                 0.08s          \n",
            "Parallel Processing  0.26s                1000            0.3%            1.8%                 0.21s                100             0.8%            1.8%                 0.47s          \n",
            "Row-by-row           0.47s                1000            0.7%            1.8%                 0.07s                100             0.6%            1.8%                 0.54s          \n",
            "Bulk Pandas          0.21s                1000            0.2%            1.8%                 0.05s                100             1.0%            1.8%                 0.26s          \n",
            "Streaming Chunks     0.37s                1000            2.1%            1.8%                 0.05s                100             1.2%            1.8%                 0.41s          \n",
            "Dask                 0.54s                1000            0.6%            1.8%                 0.50s                100             0.3%            1.8%                 1.04s          \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 100, Update Size: 10\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (100 records) ===\n",
            "Generated 100 records for initial load\n",
            "\n",
            "Sample record:\n",
            "employee_id: 0 (int)\n",
            "name: Allen Pate (str)\n",
            "email: state1852@example.com (str)\n",
            "address: 1031 Montecito Path (str)\n",
            "phone: +1-410-088-9889 (str)\n",
            "date_of_birth: 1981-05-25 (str)\n",
            "gender: Male (str)\n",
            "company: Greenville Corp (str)\n",
            "position: Salesman (str)\n",
            "salary: 135739.39 (float)\n",
            "retired: Yes (str)\n",
            "\n",
            "=== 3. Creating Subsequent Load File (10 records) ===\n",
            "Generated 10 records for subsequent load\n",
            "\n",
            "=== Method 1: PostgreSQL COPY ===\n",
            "\n",
            "Loading initial file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Allen Pate', 'state1852@example.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 100 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100 records\n",
            "Sample DB record: (0, 'Allen Pate', 'state1852@example.com', '1031 Montecito Path', '+1-410-088-9889', datetime.date(1981, 5, 25), 'Male', 'Greenville Corp', 'Salesman', Decimal('135739.39'), 'Yes', datetime.datetime(2025, 2, 26, 23, 50, 19), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "DataFrame has 14 columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "Sample row (first 3 values): [0, 'Barbar Maddox', 'characterized1948@example.com']\n",
            "Executing COPY with columns: employee_id, name, email, address, phone, date_of_birth, gender, company, position, salary, retired, valid_from, valid_to, is_current\n",
            "COPY operation successful, loaded 10 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110 records\n",
            "\n",
            "Method 1 Results:\n",
            "Initial Load: 0.02s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.3%, Max CPU: 0.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.02s (10 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Resetting table for method Parallel Processing...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 25 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100 records\n",
            "Sample DB record: (75, 'Jerica Tran', 'driven1816@outlook.com', '346 Alpine Hills', '+1-419-602-8139', datetime.date(2003, 3, 20), 'Female', 'Huntsville Corp', 'Jockey', Decimal('169710.32'), 'Yes', datetime.datetime(2025, 2, 26, 23, 50, 21, 875361), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 2 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 10 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.70%\n",
            "Max CPU Usage: 0.70%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110 records\n",
            "\n",
            "Method 2 Results:\n",
            "Initial Load: 0.20s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.3%, Max CPU: 0.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.20s (10 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.7%, Max CPU: 0.7%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Resetting table for method Row-by-row...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.70%\n",
            "Max CPU Usage: 0.70%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100 records\n",
            "Sample DB record: (0, 'Allen Pate', 'state1852@example.com', '1031 Montecito Path', '+1-410-088-9889', datetime.date(1981, 5, 25), 'Male', 'Greenville Corp', 'Salesman', Decimal('135739.39'), 'Yes', datetime.datetime(2025, 2, 26, 23, 50, 23, 903791), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110 records\n",
            "\n",
            "Method 3 Results:\n",
            "Initial Load: 0.05s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.7%, Max CPU: 0.7%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.02s (10 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Resetting table for method Bulk Pandas...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100 records\n",
            "Sample DB record: (0, 'Allen Pate', 'state1852@example.com', '1031 Montecito Path', '+1-410-088-9889', datetime.date(1981, 5, 25), 'Male', 'Greenville Corp', 'Salesman', Decimal('135739.39'), 'Yes', datetime.datetime(2025, 2, 26, 23, 50, 25, 929456), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.40%\n",
            "Max CPU Usage: 1.40%\n",
            "Min CPU Usage: 1.40%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110 records\n",
            "\n",
            "Method 4 Results:\n",
            "Initial Load: 0.03s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.02s (10 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.4%, Max CPU: 1.4%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Resetting table for method Streaming Chunks...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.90%\n",
            "Max CPU Usage: 1.90%\n",
            "Min CPU Usage: 1.90%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100 records\n",
            "Sample DB record: (0, 'Allen Pate', 'state1852@example.com', '1031 Montecito Path', '+1-410-088-9889', datetime.date(1981, 5, 25), 'Male', 'Greenville Corp', 'Salesman', Decimal('135739.39'), 'Yes', datetime.datetime(2025, 2, 26, 23, 50, 27, 957943), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110 records\n",
            "\n",
            "Method 5 Results:\n",
            "Initial Load: 0.03s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 1.9%, Max CPU: 1.9%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.02s (10 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.6%, Max CPU: 0.6%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Resetting table for method Dask...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.79 ms\n",
            "[########################################] | 100% Completed | 101.73 ms\n",
            "[########################################] | 100% Completed | 101.53 ms\n",
            "[########################################] | 100% Completed | 101.39 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Count after initial load: 100 records\n",
            "Sample DB record: (0, 'Allen Pate', 'state1852@example.com', '1031 Montecito Path', '+1-410-088-9889', datetime.date(1981, 5, 25), 'Male', 'Greenville Corp', 'Salesman', Decimal('135739.39'), 'Yes', datetime.datetime(2025, 2, 26, 23, 50, 29, 987384), datetime.datetime(2099, 12, 31, 23, 59, 59), True)\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.40 ms\n",
            "[########################################] | 100% Completed | 101.29 ms\n",
            "[########################################] | 100% Completed | 101.61 ms\n",
            "[########################################] | 100% Completed | 101.47 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.80%\n",
            "Max Memory Usage: 1.80%\n",
            "Min Memory Usage: 1.80%\n",
            "Final count: 110 records\n",
            "\n",
            "Method 6 Results:\n",
            "Initial Load: 0.49s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.4%, Max CPU: 0.4%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Update Load: 0.48s (10 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.3%, Max CPU: 0.3%\n",
            "  Avg Memory: 1.8%, Max Memory: 1.8%\n",
            "\n",
            "Results for Initial Size: 100, Update Size: 10\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PostgreSQL COPY      0.02s                100             0.3%            1.8%                 0.02s                10              0.2%            1.8%                 0.04s          \n",
            "Parallel Processing  0.20s                100             0.3%            1.8%                 0.20s                10              0.7%            1.8%                 0.39s          \n",
            "Row-by-row           0.05s                100             0.7%            1.8%                 0.02s                10              0.2%            1.8%                 0.07s          \n",
            "Bulk Pandas          0.03s                100             0.2%            1.8%                 0.02s                10              1.4%            1.8%                 0.05s          \n",
            "Streaming Chunks     0.03s                100             1.9%            1.8%                 0.02s                10              0.6%            1.8%                 0.05s          \n",
            "Dask                 0.49s                100             0.4%            1.8%                 0.48s                10              0.3%            1.8%                 0.97s          \n",
            "\n",
            "\n",
            "Comparative Summary Across All Data Sizes\n",
            "====================================================================================================\n",
            "Data Size (Initial, Update) | Best Method | Worst Method | Average Load Time\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1,000,000, 100,000 | PostgreSQL COPY (33.48s) | Row-by-row (533.94s) | 211.34s\n",
            "100,000, 10,000 | PostgreSQL COPY (3.17s) | Row-by-row (52.19s) | 21.21s\n",
            "10,000, 1,000 | PostgreSQL COPY (0.34s) | Row-by-row (5.32s) | 2.42s\n",
            "1,000, 100 | PostgreSQL COPY (0.08s) | Dask (1.04s) | 0.47s\n",
            "100, 10 | PostgreSQL COPY (0.04s) | Dask (0.97s) | 0.26s\n"
          ]
        }
      ]
    }
  ]
}