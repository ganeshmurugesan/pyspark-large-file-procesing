{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTmb9NPsu1fk",
        "outputId": "f1088044-734b-4b01-b1eb-b3a157fa3ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "default-libmysqlclient-dev is already the newest version (1.0.8).\n",
            "mysql-server is already the newest version (8.0.41-0ubuntu0.22.04.1).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.38)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: mimesis in /usr/local/lib/python3.11/dist-packages (18.0.0)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2025.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: mysql-connector-python in /usr/local/lib/python3.11/dist-packages (9.2.0)\n",
            "Requirement already satisfied: pymysql in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.12.2)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.0.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (19.0.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y mysql-server default-libmysqlclient-dev python3-dev\n",
        "\n",
        "!pip install pandas numpy sqlalchemy psutil mimesis dask \"dask[dataframe]\" tqdm mysql-connector-python pymysql"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "from typing import List, Dict\n",
        "from sqlalchemy import create_engine, text\n",
        "from multiprocessing import Pool\n",
        "import dask.dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "from mimesis.locales import Locale\n",
        "from mimesis.schema import Fieldset\n",
        "import tempfile\n",
        "import io\n",
        "import subprocess\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n",
        "import shutil\n",
        "\n",
        "\n",
        "def setup_mysql_colab():\n",
        "    \"\"\"Setup MySQL in Google Colab - set root password, enable local_infile,\n",
        "    grant FILE privilege, and set secure_file_priv to /content/mysql.\"\"\"\n",
        "    print(\"Setting up MySQL in Google Colab...\")\n",
        "\n",
        "    # Install MySQL if needed\n",
        "    if not os.path.exists(\"/usr/bin/mysql\"):\n",
        "        print(\"Installing MySQL...\")\n",
        "        get_ipython().system(\"apt-get update\")\n",
        "        get_ipython().system(\"apt-get install -y mysql-server python3-dev default-libmysqlclient-dev\")\n",
        "        get_ipython().system(\"pip install PyMySQL\")\n",
        "\n",
        "    # Start MySQL\n",
        "    get_ipython().system(\"service mysql start\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Set root password using sudo\n",
        "    try:\n",
        "        cmd = \"sudo mysql -u root -e \\\"ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';\\\"\"\n",
        "        get_ipython().system(cmd)\n",
        "        get_ipython().system(\"sudo mysql -u root -ppassword -e 'FLUSH PRIVILEGES;'\")\n",
        "        print(\"Root password set using sudo.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting root password with sudo: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Create Database and User with proper privileges including FILE privilege\n",
        "    try:\n",
        "        conn = mysql.connector.connect(user='root', password='password', host='localhost')\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"CREATE DATABASE IF NOT EXISTS employees;\")\n",
        "        cursor.execute(\"CREATE USER IF NOT EXISTS 'mysql_user'@'localhost' IDENTIFIED BY 'password';\")\n",
        "        cursor.execute(\"GRANT ALL PRIVILEGES ON employees.* TO 'mysql_user'@'localhost';\")\n",
        "        # Grant FILE privilege to allow file-based operations\n",
        "        cursor.execute(\"GRANT FILE ON *.* TO 'mysql_user'@'localhost';\")\n",
        "        cursor.execute(\"FLUSH PRIVILEGES;\")\n",
        "        conn.commit()\n",
        "        print(\"Database and user 'mysql_user' created/configured successfully.\")\n",
        "    except mysql.connector.Error as err:\n",
        "        print(f\"Error creating database/user: {err}\")\n",
        "        raise\n",
        "    finally:\n",
        "        if 'cursor' in locals():\n",
        "            cursor.close()\n",
        "        if 'conn' in locals():\n",
        "            conn.close()\n",
        "\n",
        "    # Create /content/mysql directory for secure file operations\n",
        "    get_ipython().system(\"mkdir -p /content/mysql\")\n",
        "\n",
        "    # Update MySQL configuration to set secure_file_priv to /content/mysql\n",
        "    # Replace existing secure_file_priv line if present; otherwise, append it.\n",
        "    get_ipython().system(\"sudo sed -i '/^secure_file_priv/s|=.*|= /content/mysql|' /etc/mysql/mysql.conf.d/mysqld.cnf\")\n",
        "    get_ipython().system(\"sudo bash -c 'grep -q \\\"^secure_file_priv\\\" /etc/mysql/mysql.conf.d/mysqld.cnf || echo \\\"secure_file_priv = /content/mysql\\\" >> /etc/mysql/mysql.conf.d/mysqld.cnf'\")\n",
        "\n",
        "    # Update bind-address and restart MySQL\n",
        "    get_ipython().system(\"sudo sed -i 's/bind-address.*/bind-address = 0.0.0.0/' /etc/mysql/mysql.conf.d/mysqld.cnf\")\n",
        "    get_ipython().system(\"service mysql restart\")\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Enable LOCAL INFILE on the server\n",
        "    get_ipython().system(\"sudo mysql -u root -ppassword -e \\\"SET GLOBAL local_infile = 1;\\\"\")\n",
        "    print(\"LOCAL INFILE enabled.\")\n",
        "\n",
        "    # Update connection string to enable local_infile on the client side\n",
        "    connection_string = \"mysql+pymysql://mysql_user:password@localhost:3306/employees?local_infile=1\"\n",
        "    engine = create_engine(connection_string)\n",
        "\n",
        "    # Test connection to ensure everything is set up correctly\n",
        "    max_attempts = 30\n",
        "    attempt = 0\n",
        "    while attempt < max_attempts:\n",
        "        try:\n",
        "            print(f\"Attempting to connect to database... (Attempt {attempt + 1}/{max_attempts})\")\n",
        "            with engine.connect() as connection:\n",
        "                connection.execute(text(\"SELECT 1\"))\n",
        "            print(\"Successfully connected to MySQL!\")\n",
        "            return connection_string\n",
        "        except Exception as e:\n",
        "            print(f\"Connection attempt failed: {str(e)}\")\n",
        "            attempt += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    raise Exception(\"Failed to connect to MySQL after maximum attempts\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def setup_database(engine):\n",
        "    \"\"\"Create SCD Type 2 table schema\"\"\"\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS employees (\n",
        "                employee_id INT,\n",
        "                name VARCHAR(100),\n",
        "                email VARCHAR(100),\n",
        "                address TEXT,\n",
        "                phone VARCHAR(50),\n",
        "                date_of_birth DATE,\n",
        "                gender VARCHAR(10),\n",
        "                company VARCHAR(100),\n",
        "                position VARCHAR(100),\n",
        "                salary DECIMAL(10,2),\n",
        "                retired VARCHAR(3),\n",
        "                valid_from TIMESTAMP,\n",
        "                valid_to TIMESTAMP,\n",
        "                is_current BOOLEAN,\n",
        "                PRIMARY KEY (employee_id, valid_from)\n",
        "            )\n",
        "        \"\"\"))\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "def generate_data(row_count: int) -> pd.DataFrame:\n",
        "    \"\"\"Generate synthetic data using Mimesis Fieldset\"\"\"\n",
        "    fieldset = Fieldset(locale=Locale.EN)\n",
        "\n",
        "    # Generate all fields at once using Fieldset\n",
        "    employee_ids = list(range(row_count))\n",
        "    names = fieldset(\"full_name\", i=row_count)\n",
        "    emails = fieldset(\"email\", i=row_count)\n",
        "    addresses = fieldset(\"address\", i=row_count)\n",
        "    phones = fieldset(\"telephone\", i=row_count)\n",
        "    dates = [str(date.isoformat()) for date in fieldset(\"date\", start=1950, end=2005, i=row_count)]\n",
        "    genders = np.random.choice([\"Male\", \"Female\"], size=row_count).tolist()\n",
        "    cities = fieldset(\"city\", i=row_count)\n",
        "    positions = fieldset(\"occupation\", i=row_count)\n",
        "    salaries = np.round(np.random.uniform(30000, 200000, row_count), 2).tolist()\n",
        "    retired = np.random.choice([\"Yes\", \"No\"], size=row_count).tolist()\n",
        "\n",
        "    # Create records using list comprehension with zip\n",
        "    records = [\n",
        "        {\n",
        "            \"employee_id\": emp_id,\n",
        "            \"name\": name,\n",
        "            \"email\": email,\n",
        "            \"address\": address,\n",
        "            \"phone\": phone,\n",
        "            \"date_of_birth\": dob,\n",
        "            \"gender\": gender,\n",
        "            \"company\": f\"{city} Corp\",\n",
        "            \"position\": position,\n",
        "            \"salary\": salary,\n",
        "            \"retired\": retired_status\n",
        "        }\n",
        "        for emp_id, name, email, address, phone, dob, gender, city, position, salary, retired_status\n",
        "        in zip(employee_ids, names, emails, addresses, phones, dates, genders,\n",
        "               cities, positions, salaries, retired)\n",
        "    ]\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "def identify_changes(new_df: pd.DataFrame, engine) -> pd.DataFrame:\n",
        "    \"\"\"Compare new data with existing records and identify changes\"\"\"\n",
        "    current_records = pd.read_sql(\n",
        "        \"\"\"\n",
        "        SELECT * FROM employees\n",
        "        WHERE is_current = true\n",
        "        \"\"\",\n",
        "        engine\n",
        "    )\n",
        "\n",
        "    if len(current_records) > 0:\n",
        "        merged = new_df.merge(\n",
        "            current_records,\n",
        "            on='employee_id',\n",
        "            how='left',\n",
        "            suffixes=('_new', '_current')\n",
        "        )\n",
        "\n",
        "        changed_mask = (\n",
        "            (merged['name_new'] != merged['name_current']) |\n",
        "            (merged['email_new'] != merged['email_current']) |\n",
        "            (merged['address_new'] != merged['address_current']) |\n",
        "            (merged['phone_new'] != merged['phone_current']) |\n",
        "            (merged['position_new'] != merged['position_current']) |\n",
        "            (merged['salary_new'] != merged['salary_current'])\n",
        "        )\n",
        "\n",
        "        new_mask = merged['name_current'].isna()\n",
        "\n",
        "        new_df['change_type'] = 'no_change'\n",
        "        new_df.loc[new_mask, 'change_type'] = 'insert'\n",
        "        new_df.loc[changed_mask & ~new_mask, 'change_type'] = 'update'\n",
        "    else:\n",
        "        new_df['change_type'] = 'insert'\n",
        "\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def apply_scd2_changes(df: pd.DataFrame, engine) -> pd.DataFrame:\n",
        "    \"\"\"Apply SCD Type 2 changes to the data\"\"\"\n",
        "    current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Ensure datetime format for 'valid_from' and 'valid_to'\n",
        "    df['valid_from'] = current_timestamp\n",
        "    # Use a more compatible \"far future\" date\n",
        "    df['valid_to'] = pd.to_datetime('2038-01-19 03:14:07').strftime('%Y-%m-%d %H:%M:%S')  # Changed date\n",
        "    df['is_current'] = True\n",
        "\n",
        "    # Ensure date fields are correctly formatted\n",
        "    if 'date_of_birth' in df.columns:\n",
        "        df['date_of_birth'] = pd.to_datetime(df['date_of_birth'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    updates = df[df['change_type'] == 'update']\n",
        "    if not updates.empty:\n",
        "        with engine.begin() as conn:\n",
        "            employee_ids = tuple(updates['employee_id'].tolist())\n",
        "            if len(employee_ids) == 1:\n",
        "                query = text(\"\"\"\n",
        "                    UPDATE employees\n",
        "                    SET valid_to = :valid_to,\n",
        "                        is_current = FALSE\n",
        "                    WHERE employee_id = :employee_id\n",
        "                    AND is_current = TRUE\n",
        "                \"\"\")\n",
        "                conn.execute(query, {\"valid_to\": current_timestamp, \"employee_id\": employee_ids[0]})\n",
        "            else:\n",
        "                query = text(\"\"\"\n",
        "                    UPDATE employees\n",
        "                    SET valid_to = :valid_to,\n",
        "                        is_current = FALSE\n",
        "                    WHERE employee_id IN :employee_ids\n",
        "                    AND is_current = TRUE\n",
        "                \"\"\")\n",
        "                conn.execute(query, {\"valid_to\": current_timestamp, \"employee_ids\": employee_ids})\n",
        "\n",
        "    return df.drop(columns=['change_type'])\n",
        "\n",
        "\n",
        "\n",
        "def reset_table(engine):\n",
        "    \"\"\"Drop and recreate the employees table\"\"\"\n",
        "    try:\n",
        "        with engine.connect() as conn:\n",
        "            conn.execute(text(\"DROP TABLE IF EXISTS employees\"))\n",
        "            conn.commit()\n",
        "\n",
        "        # Recreate the table schema\n",
        "        setup_database(engine)\n",
        "\n",
        "        print(\"Table dropped and recreated successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error resetting table: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Modify truncate_table to use reset_table instead\n",
        "def truncate_table(engine):\n",
        "    \"\"\"Use reset_table instead of truncate for cleaner state\"\"\"\n",
        "    reset_table(engine)\n",
        "\n",
        "def monitor_resources(interval, stats):\n",
        "    \"\"\"Monitor CPU and memory usage\"\"\"\n",
        "    while not stats['stop']:\n",
        "        stats['cpu'].append(psutil.cpu_percent(interval=None))\n",
        "        stats['memory'].append(psutil.virtual_memory().percent)\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def print_resource_stats(stats):\n",
        "    \"\"\"Print resource usage statistics\"\"\"\n",
        "    print(\"\\nResource Usage Statistics:\")\n",
        "    print(f\"Average CPU Usage: {sum(stats['cpu']) / len(stats['cpu']):.2f}%\")\n",
        "    print(f\"Max CPU Usage: {max(stats['cpu']):.2f}%\")\n",
        "    print(f\"Min CPU Usage: {min(stats['cpu']):.2f}%\")\n",
        "    print(f\"Average Memory Usage: {sum(stats['memory']) / len(stats['memory']):.2f}%\")\n",
        "    print(f\"Max Memory Usage: {max(stats['memory']):.2f}%\")\n",
        "    print(f\"Min Memory Usage: {min(stats['memory']):.2f}%\")\n",
        "\n",
        "\n",
        "def monitor_performance(func):\n",
        "    \"\"\"Decorator to monitor performance of loading methods\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        stats = {'cpu': [], 'memory': [], 'stop': False}\n",
        "\n",
        "        # Start monitoring thread\n",
        "        monitor_thread = threading.Thread(target=monitor_resources, args=(1, stats))\n",
        "        monitor_thread.start()\n",
        "\n",
        "        try:\n",
        "            # Execute the loading function\n",
        "            start_time = time.time()\n",
        "            func(*args, **kwargs)\n",
        "            duration = time.time() - start_time\n",
        "\n",
        "            # Stop monitoring\n",
        "            stats['stop'] = True\n",
        "            monitor_thread.join()\n",
        "\n",
        "            # Calculate resource statistics\n",
        "            resource_stats = {\n",
        "                'duration': duration,\n",
        "                'avg_cpu': sum(stats['cpu']) / len(stats['cpu']) if stats['cpu'] else 0,\n",
        "                'max_cpu': max(stats['cpu']) if stats['cpu'] else 0,\n",
        "                'avg_memory': sum(stats['memory']) / len(stats['memory']) if stats['memory'] else 0,\n",
        "                'max_memory': max(stats['memory']) if stats['memory'] else 0\n",
        "            }\n",
        "\n",
        "            print_resource_stats(stats)\n",
        "            return duration, resource_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            stats['stop'] = True\n",
        "            monitor_thread.join()\n",
        "            raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "@monitor_performance\n",
        "def load_row_by_row(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data row by row with SCD Type 2\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        for _, row in df.iterrows():\n",
        "            conn.execute(\n",
        "                text(\"\"\"\n",
        "                    INSERT INTO employees\n",
        "                    VALUES (:employee_id, :name, :email, :address, :phone,\n",
        "                           :date_of_birth, :gender, :company, :position,\n",
        "                           :salary, :retired, :valid_from, :valid_to, :is_current)\n",
        "                \"\"\"),\n",
        "                row.to_dict()\n",
        "            )\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "\n",
        "@monitor_performance\n",
        "def load_bulk_pandas(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data using pandas bulk insert\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "    df.to_sql('employees', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "\n",
        "@monitor_performance\n",
        "def load_streaming_chunks(df: pd.DataFrame, engine, chunk_size=1000):\n",
        "    \"\"\"Load data in chunks\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "\n",
        "    for chunk_start in range(0, len(df), chunk_size):\n",
        "        chunk = df.iloc[chunk_start:chunk_start + chunk_size]\n",
        "        chunk.to_sql('employees', engine, if_exists='append', index=False, method='multi')\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "\n",
        "def parallel_worker(chunk_data):\n",
        "    \"\"\"Worker function for parallel processing with proper connection and error handling\"\"\"\n",
        "    try:\n",
        "        # Create a new engine for each worker to avoid connection sharing issues\n",
        "        engine = create_engine(\"mysql+pymysql://mysql_user:password@localhost:3306/employees\")\n",
        "\n",
        "        # Use with context to ensure proper resource management\n",
        "        with engine.begin() as conn:\n",
        "            # Use if_exists='append' to ensure we don't recreate the table\n",
        "            chunk_data.to_sql('employees', conn, if_exists='append', index=False, method='multi')\n",
        "\n",
        "        return len(chunk_data)  # Return the number of records processed\n",
        "    except Exception as e:\n",
        "        print(f\"Worker error: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "@monitor_performance\n",
        "def load_parallel(df: pd.DataFrame, engine, num_processes=4):\n",
        "    \"\"\"Load data using parallel processing with proper error handling\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        df = identify_changes(df, engine)\n",
        "        df = apply_scd2_changes(df, engine)  # Apply SCD2 changes\n",
        "\n",
        "        # Split the dataframe into chunks\n",
        "        chunks = np.array_split(df, num_processes)\n",
        "        print(f\"Split data into {len(chunks)} chunks of approximately {len(df) // num_processes} records each\")\n",
        "\n",
        "        # Use Pool to process chunks in parallel with proper error handling\n",
        "        with Pool(num_processes) as pool:\n",
        "            try:\n",
        "                # Use map_async with get() to catch worker exceptions\n",
        "                results = pool.map_async(parallel_worker, chunks)\n",
        "                processed_counts = results.get()  # This will raise any exceptions from workers\n",
        "                total_processed = sum(processed_counts) if processed_counts else 0\n",
        "                print(f\"Successfully processed {total_processed} records in parallel\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in parallel processing: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in load_parallel: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "\n",
        "@monitor_performance\n",
        "def load_dask(df: pd.DataFrame, engine, npartitions=4):\n",
        "    \"\"\"Load data using Dask\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)  # Apply SCD2 changes\n",
        "\n",
        "    ddf = dd.from_pandas(df, npartitions=npartitions)\n",
        "    with ProgressBar():\n",
        "        for partition in ddf.partitions:\n",
        "            partition.compute().to_sql('employees', engine, if_exists='append', index=False)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "\n",
        "@monitor_performance\n",
        "def load_mysql_load_data_modified(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data using MySQL LOAD DATA INFILE command - modified for Colab environments.\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        df = identify_changes(df, engine)\n",
        "        df = apply_scd2_changes(df, engine)\n",
        "\n",
        "        # Create a copy of the dataframe to avoid modifying the original\n",
        "        copy_df = df.copy()\n",
        "\n",
        "        # Convert timestamp columns to proper format\n",
        "        copy_df['valid_from'] = pd.to_datetime(copy_df['valid_from']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        copy_df['valid_to'] = pd.to_datetime(copy_df['valid_to']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Convert date_of_birth to date format\n",
        "        copy_df['date_of_birth'] = pd.to_datetime(copy_df['date_of_birth']).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "        # Handle boolean values - MySQL uses 1/0 for booleans\n",
        "        copy_df['is_current'] = copy_df['is_current'].map({True: 1, False: 0})\n",
        "\n",
        "        # Escape special characters in text fields\n",
        "        text_cols = ['name', 'email', 'address', 'phone', 'gender', 'company', 'position', 'retired']\n",
        "        for col in text_cols:\n",
        "            copy_df[col] = copy_df[col].astype(str).str.replace('\\\\', '\\\\\\\\')\n",
        "            copy_df[col] = copy_df[col].str.replace(',', '\\\\,')\n",
        "            copy_df[col] = copy_df[col].str.replace('\\t', ' ')\n",
        "            copy_df[col] = copy_df[col].str.replace('\\n', ' ')\n",
        "            copy_df[col] = copy_df[col].str.replace('\"', '\\\\\"')\n",
        "\n",
        "        # Ensure numeric types\n",
        "        copy_df['employee_id'] = copy_df['employee_id'].astype(int)\n",
        "        copy_df['salary'] = copy_df['salary'].astype(float)\n",
        "\n",
        "        # Create a temporary CSV file\n",
        "        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.csv') as f:\n",
        "            csv_path = f.name\n",
        "            copy_df.to_csv(\n",
        "                f,\n",
        "                index=False,\n",
        "                sep=',',\n",
        "                quoting=csv.QUOTE_ALL,  # Quote all fields\n",
        "                quotechar='\"',\n",
        "                doublequote=True,\n",
        "                lineterminator='\\n'\n",
        "            )\n",
        "\n",
        "        # First, check if we can determine MySQL's secure-file-priv setting\n",
        "        try:\n",
        "            with engine.connect() as conn:\n",
        "                result = conn.execute(text(\"SHOW VARIABLES LIKE 'secure_file_priv'\"))\n",
        "                row = result.fetchone()\n",
        "                if row:\n",
        "                    secure_file_priv = row[1]\n",
        "                else:\n",
        "                    secure_file_priv = None\n",
        "                print(f\"MySQL secure_file_priv: {secure_file_priv}\")\n",
        "        except Exception:\n",
        "            secure_file_priv = None\n",
        "            print(\"Could not determine secure_file_priv setting, will try local file load\")\n",
        "\n",
        "        secure_file_path = csv_path\n",
        "        if secure_file_priv and secure_file_priv.lower() != 'null':\n",
        "            if secure_file_priv:\n",
        "                # Copy file to secure directory\n",
        "                dest_path = os.path.join(secure_file_priv, os.path.basename(csv_path))\n",
        "                shutil.copy2(csv_path, dest_path)\n",
        "                secure_file_path = dest_path\n",
        "\n",
        "                # Try to set permissions if needed\n",
        "                try:\n",
        "                    os.chmod(secure_file_path, 0o644)\n",
        "                except Exception:\n",
        "                    print(\"Warning: Could not change file permissions\")\n",
        "\n",
        "        try:\n",
        "            # Create a database connection\n",
        "            conn = mysql.connector.connect(\n",
        "                host=\"localhost\",\n",
        "                user=\"mysql_user\",\n",
        "                password=\"password\",\n",
        "                database=\"employees\"\n",
        "            )\n",
        "\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            # Get the column names string for the LOAD DATA INFILE command\n",
        "            columns = \", \".join([f\"`{col}`\" for col in copy_df.columns])\n",
        "\n",
        "            # Different approach based on secure_file_priv\n",
        "            if not secure_file_priv or secure_file_priv.lower() == 'null':\n",
        "                # If secure_file_priv is NULL, fall back to direct INSERT\n",
        "                print(\"LOAD DATA INFILE disabled, falling back to batch INSERT\")\n",
        "                raise mysql.connector.Error(\"LOAD DATA INFILE disabled\") # Force the fallback\n",
        "\n",
        "            else:\n",
        "                # Try LOCAL first as it's more compatible across environments\n",
        "                load_data_query = f\"\"\"\n",
        "                LOAD DATA LOCAL INFILE '{secure_file_path}'\n",
        "                INTO TABLE employees\n",
        "                FIELDS TERMINATED BY ','\n",
        "                ENCLOSED BY '\"'\n",
        "                LINES TERMINATED BY '\\\\n'\n",
        "                IGNORE 1 ROWS\n",
        "                ({columns});\n",
        "                \"\"\"\n",
        "\n",
        "                try:\n",
        "                    print(\"Trying LOAD DATA LOCAL INFILE...\")\n",
        "                    cursor.execute(load_data_query)\n",
        "                    conn.commit()\n",
        "                except mysql.connector.Error as e: # Catch MySQL errors specifically\n",
        "                    print(f\"LOCAL INFILE failed: {str(e)}\")\n",
        "                    print(\"Trying standard LOAD DATA INFILE...\")\n",
        "\n",
        "                    # Try without LOCAL\n",
        "                    load_data_query = load_data_query.replace(\"LOCAL INFILE\", \"INFILE\")\n",
        "                    cursor.execute(load_data_query)\n",
        "                    conn.commit()\n",
        "\n",
        "                print(f\"LOAD DATA INFILE operation successful, loaded {len(copy_df)} records\")\n",
        "\n",
        "        except mysql.connector.Error as e:  # Catch MySQL errors *here*\n",
        "            print(f\"MySQL Error (LOAD DATA or fallback): {str(e)}\")\n",
        "            # Fallback to batch INSERT\n",
        "            print(\"Falling back to batch INSERT...\")\n",
        "\n",
        "            batch_size = 1000\n",
        "            for i in range(0, len(copy_df), batch_size):\n",
        "                batch = copy_df.iloc[i:i+batch_size]\n",
        "\n",
        "                # Generate placeholders for parameterized query\n",
        "                placeholders = \", \".join([\"%s\"] * len(copy_df.columns))\n",
        "                cols = \", \".join([f\"`{col}`\" for col in copy_df.columns])\n",
        "\n",
        "                # Convert dataframe to list of tuples for executemany\n",
        "                rows = [tuple(x) for x in batch.to_numpy()]\n",
        "\n",
        "                # Execute batch insert\n",
        "                insert_query = f\"INSERT INTO employees ({cols}) VALUES ({placeholders})\"\n",
        "                cursor.executemany(insert_query, rows)\n",
        "                conn.commit()\n",
        "\n",
        "            print(f\"Batch INSERT operation successful, loaded {len(copy_df)} records\")\n",
        "\n",
        "\n",
        "        finally: # Use finally to ensure cleanup *always* happens\n",
        "            if 'cursor' in locals():\n",
        "                cursor.close()\n",
        "            if 'conn' in locals():\n",
        "                conn.close()\n",
        "\n",
        "            # Remove temporary files\n",
        "            try:\n",
        "                os.remove(csv_path)\n",
        "                if secure_file_path != csv_path and os.path.exists(secure_file_path):\n",
        "                    os.remove(secure_file_path)\n",
        "            except Exception:\n",
        "                print(\"Warning: Could not remove temporary files\")\n",
        "\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in MySQL LOAD DATA: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "def main():\n",
        "    # 1. Setup Database\n",
        "    print(\"\\n=== 1. Setting up MySQL Database ===\")\n",
        "    connection_string = setup_mysql_colab()\n",
        "    engine = create_engine(connection_string)\n",
        "    setup_database(engine)\n",
        "\n",
        "    # Define test data sizes\n",
        "    data_sizes = [\n",
        "        (1000000, 100000),\n",
        "        (100000, 10000),\n",
        "        (10000, 1000),\n",
        "        (1000, 100),\n",
        "        (100, 10)\n",
        "    ]\n",
        "\n",
        "    # Define the loading methods\n",
        "    methods = [\n",
        "        (load_mysql_load_data_modified, \"MySQL LOAD DATA\"),  # Keep the modified version\n",
        "        (load_parallel, \"Parallel Processing\"),\n",
        "        (load_row_by_row, \"Row-by-row\"),\n",
        "        (load_bulk_pandas, \"Bulk Pandas\"),\n",
        "        (load_streaming_chunks, \"Streaming Chunks\"),\n",
        "        (load_dask, \"Dask\")\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "    num_runs = 1 # Number of runs for each method\n",
        "\n",
        "    # Loop through different data sizes\n",
        "    for initial_size, update_size in data_sizes:\n",
        "        print(f\"\\n\\n================================================\")\n",
        "        print(f\"Testing with Initial Size: {initial_size}, Update Size: {update_size}\")\n",
        "        print(f\"================================================\")\n",
        "\n",
        "        # Reset the table at the beginning of each data size test\n",
        "        print(\"\\nResetting table for new data size test...\")\n",
        "        reset_table(engine)\n",
        "\n",
        "        # 2. Create Initial Load File\n",
        "        print(f\"\\n=== 2. Creating Initial Load File ({initial_size} records) ===\")\n",
        "        initial_df = generate_data(initial_size)\n",
        "        print(f\"Generated {len(initial_df)} records for initial load\")\n",
        "\n",
        "        # 3. Create Subsequent Load File\n",
        "        print(f\"\\n=== 3. Creating Subsequent Load File ({update_size} records) ===\")\n",
        "        update_df = generate_data(update_size)\n",
        "        print(f\"Generated {len(update_df)} records for subsequent load\")\n",
        "\n",
        "\n",
        "        # Execute methods sequentially\n",
        "        for idx, (method, name) in enumerate(methods, start=1):\n",
        "            print(f\"\\n=== Method {idx}: {name} ===\")\n",
        "            method_results = []\n",
        "\n",
        "            for run in range(num_runs):\n",
        "                print(f\"\\nRun {run + 1} of {num_runs}\")\n",
        "\n",
        "                # Reset table before each method run (except first run of first method)\n",
        "                if idx > 1 or run > 0 :\n",
        "                    print(f\"\\nResetting table for method {name}, run {run+1}...\")\n",
        "                    reset_table(engine)\n",
        "\n",
        "                try:\n",
        "                    # 4. Load Initial File\n",
        "                    print(f\"\\nLoading initial file...\")\n",
        "                    initial_duration, initial_stats = method(initial_df.copy(), engine) # Pass a copy\n",
        "\n",
        "                    # Get count after initial load\n",
        "                    with engine.connect() as conn:\n",
        "                        initial_count = conn.execute(text(\"SELECT COUNT(*) FROM employees\")).scalar()\n",
        "                        print(f\"Count after initial load: {initial_count} records\")\n",
        "\n",
        "                    # 5. Load Subsequent File\n",
        "                    print(f\"\\nLoading subsequent file...\")\n",
        "                    update_duration, update_stats = method(update_df.copy(), engine) # Pass a copy\n",
        "\n",
        "                    # Get final count\n",
        "                    with engine.connect() as conn:\n",
        "                        final_count = conn.execute(text(\"SELECT COUNT(*) FROM employees\")).scalar()\n",
        "                        print(f\"Final count: {final_count} records\")\n",
        "\n",
        "                    result = {\n",
        "                        'Method': name,\n",
        "                        'Initial Size': initial_size,\n",
        "                        'Update Size': update_size,\n",
        "                        'Initial Load Time': initial_duration,\n",
        "                        'Initial Records': initial_count,\n",
        "                        'Initial Avg CPU': initial_stats['avg_cpu'],\n",
        "                        'Initial Max CPU': initial_stats['max_cpu'],\n",
        "                        'Initial Avg Memory': initial_stats['avg_memory'],\n",
        "                        'Initial Max Memory': initial_stats['max_memory'],\n",
        "                        'Update Load Time': update_duration,\n",
        "                        'Update Records': final_count - initial_count,\n",
        "                        'Update Avg CPU': update_stats['avg_cpu'],\n",
        "                        'Update Max CPU': update_stats['max_cpu'],\n",
        "                        'Update Avg Memory': update_stats['avg_memory'],\n",
        "                        'Update Max Memory': update_stats['max_memory'],\n",
        "                        'Final Records': final_count,\n",
        "                        'Total Time': initial_duration + update_duration\n",
        "                    }\n",
        "                    method_results.append(result)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in {name} method: {str(e)}\")\n",
        "                    method_results.append({\n",
        "                        'Method': name,\n",
        "                        'Initial Size': initial_size,\n",
        "                        'Update Size': update_size,\n",
        "                        'Initial Load Time': 'Failed',\n",
        "                        'Initial Records': 'Failed',\n",
        "                        'Initial Avg CPU': 'Failed',\n",
        "                        'Initial Max CPU': 'Failed',\n",
        "                        'Initial Avg Memory': 'Failed',\n",
        "                        'Initial Max Memory': 'Failed',\n",
        "                        'Update Load Time': 'Failed',\n",
        "                        'Update Records': 'Failed',\n",
        "                        'Update Avg CPU': 'Failed',\n",
        "                        'Update Max CPU': 'Failed',\n",
        "                        'Update Avg Memory': 'Failed',\n",
        "                        'Update Max Memory': 'Failed',\n",
        "                        'Final Records': 'Failed',\n",
        "                        'Total Time': 'Failed'\n",
        "                    })\n",
        "            # Calculate average results for the method\n",
        "            avg_result = {}\n",
        "            successful_runs = [r for r in method_results if r['Total Time'] != 'Failed']\n",
        "            if successful_runs:\n",
        "                for key in successful_runs[0].keys():\n",
        "                    if isinstance(successful_runs[0][key], (int, float)):\n",
        "                        avg_result[key] = sum(r[key] for r in successful_runs) / len(successful_runs)\n",
        "                    else:\n",
        "                        avg_result[key] = successful_runs[0][key]  # Use first run value for non-numeric\n",
        "\n",
        "                if f\"{initial_size}_{update_size}\" not in all_results:\n",
        "                    all_results[f\"{initial_size}_{update_size}\"] = []\n",
        "                all_results[f\"{initial_size}_{update_size}\"].append(avg_result)\n",
        "                print(f\"\\nAverage Results for Method {idx} ({name}) after {num_runs} runs:\")\n",
        "                print(avg_result)\n",
        "\n",
        "            else:\n",
        "                print(f\"All runs for method {name} failed.\")\n",
        "                if f\"{initial_size}_{update_size}\" not in all_results:\n",
        "                     all_results[f\"{initial_size}_{update_size}\"] = []\n",
        "                all_results[f\"{initial_size}_{update_size}\"].append({\n",
        "                    'Method': name,\n",
        "                    'Initial Size': initial_size,\n",
        "                    'Update Size': update_size,\n",
        "                    'Initial Load Time': 'Failed',\n",
        "                    'Initial Records': 'Failed',\n",
        "                    'Initial Avg CPU': 'Failed',\n",
        "                    'Initial Max CPU': 'Failed',\n",
        "                    'Initial Avg Memory': 'Failed',\n",
        "                    'Initial Max Memory': 'Failed',\n",
        "                    'Update Load Time': 'Failed',\n",
        "                    'Update Records': 'Failed',\n",
        "                    'Update Avg CPU': 'Failed',\n",
        "                    'Update Max CPU': 'Failed',\n",
        "                    'Update Avg Memory': 'Failed',\n",
        "                    'Update Max Memory': 'Failed',\n",
        "                    'Final Records': 'Failed',\n",
        "                    'Total Time': 'Failed'\n",
        "                })\n",
        "\n",
        "        # Print results for current data size (after all methods and runs)\n",
        "        print(f\"\\nResults for Initial Size: {initial_size}, Update Size: {update_size}\")\n",
        "        print(\"=\" * 140)\n",
        "        headers = [\n",
        "            'Method', 'Initial Load Time', 'Initial Records', 'Initial Avg CPU', 'Initial Max Memory',\n",
        "            'Update Load Time', 'Update Records', 'Update Avg CPU', 'Update Max Memory',\n",
        "            'Total Time'\n",
        "        ]\n",
        "        row_format = \"{:<20} {:<20} {:<15} {:<15} {:<20} {:<20} {:<15} {:<15} {:<20} {:<15}\"\n",
        "        print(row_format.format(*headers))\n",
        "        print(\"-\" * 140)\n",
        "        for result in all_results[f\"{initial_size}_{update_size}\"]:\n",
        "            print(row_format.format(\n",
        "                result['Method'],\n",
        "                f\"{result['Initial Load Time']:.2f}s\" if isinstance(result['Initial Load Time'], (int, float)) else result['Initial Load Time'],\n",
        "                str(int(result['Initial Records'])) if isinstance(result['Initial Records'], (int, float)) else result['Initial Records'],\n",
        "                f\"{result['Initial Avg CPU']:.1f}%\" if isinstance(result['Initial Avg CPU'], (int, float)) else result['Initial Avg CPU'],\n",
        "                f\"{result['Initial Max Memory']:.1f}%\" if isinstance(result['Initial Max Memory'], (int, float)) else result['Initial Max Memory'],\n",
        "                f\"{result['Update Load Time']:.2f}s\" if isinstance(result['Update Load Time'],(int, float)) else result['Update Load Time'],\n",
        "                str(int(result['Update Records'])) if isinstance(result['Update Records'], (int, float)) else result['Update Records'],\n",
        "                f\"{result['Update Avg CPU']:.1f}%\" if isinstance(result['Update Avg CPU'], (int, float)) else result['Update Avg CPU'],\n",
        "                f\"{result['Update Max Memory']:.1f}%\" if isinstance(result['Update Max Memory'], (int, float)) else result['Update Max Memory'],\n",
        "                f\"{result['Total Time']:.2f}s\" if isinstance(result['Total Time'], (int, float)) else result['Total Time']\n",
        "            ))\n",
        "\n",
        "\n",
        "    # Print comparative summary across all data sizes\n",
        "    print(\"\\n\\nComparative Summary Across All Data Sizes\")\n",
        "    print(\"=\" * 100)\n",
        "    print(\"Data Size (Initial, Update) | Best Method | Worst Method | Average Load Time\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K00S0Azj0hC0",
        "outputId": "ba429f1b-5b00-4c41-c5d9-dc9924343e5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 1. Setting up MySQL Database ===\n",
            "Setting up MySQL in Google Colab...\n",
            " * Starting MySQL database server mysqld\n",
            "su: warning: cannot change directory to /nonexistent: No such file or directory\n",
            "   ...done.\n",
            "mysql: [Warning] Using a password on the command line interface can be insecure.\n",
            "Root password set using sudo.\n",
            "Database and user 'mysql_user' created/configured successfully.\n",
            " * Stopping MySQL database server mysqld\n",
            "   ...done.\n",
            " * Starting MySQL database server mysqld\n",
            "su: warning: cannot change directory to /nonexistent: No such file or directory\n",
            "   ...done.\n",
            "mysql: [Warning] Using a password on the command line interface can be insecure.\n",
            "LOCAL INFILE enabled.\n",
            "Attempting to connect to database... (Attempt 1/30)\n",
            "Successfully connected to MySQL!\n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 1000000, Update Size: 100000\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (1000000 records) ===\n",
            "Generated 1000000 records for initial load\n",
            "\n",
            "=== 3. Creating Subsequent Load File (100000 records) ===\n",
            "Generated 100000 records for subsequent load\n",
            "\n",
            "=== Method 1: MySQL LOAD DATA ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Loading initial file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 1000000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.48%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.10%\n",
            "Average Memory Usage: 2.04%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 1.90%\n",
            "Count after initial load: 1000000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 100000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.91%\n",
            "Max CPU Usage: 3.30%\n",
            "Min CPU Usage: 1.20%\n",
            "Average Memory Usage: 1.97%\n",
            "Max Memory Usage: 2.20%\n",
            "Min Memory Usage: 1.90%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Average Results for Method 1 (MySQL LOAD DATA) after 1 runs:\n",
            "{'Method': 'MySQL LOAD DATA', 'Initial Size': 1000000.0, 'Update Size': 100000.0, 'Initial Load Time': 42.9843053817749, 'Initial Records': 1000000.0, 'Initial Avg CPU': 1.4761904761904767, 'Initial Max CPU': 2.9, 'Initial Avg Memory': 2.042857142857142, 'Initial Max Memory': 2.1, 'Update Load Time': 29.1832492351532, 'Update Records': 100000.0, 'Update Avg CPU': 1.907142857142857, 'Update Max CPU': 3.3, 'Update Avg Memory': 1.967857142857143, 'Update Max Memory': 2.2, 'Final Records': 1100000.0, 'Total Time': 72.1675546169281}\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Parallel Processing, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into 4 chunks of approximately 250000 records each\n",
            "Successfully processed 1000000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 4.13%\n",
            "Max CPU Usage: 5.90%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 4.54%\n",
            "Max Memory Usage: 7.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 25000 records each\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 2.59%\n",
            "Max CPU Usage: 5.40%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 2.12%\n",
            "Max Memory Usage: 2.50%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Average Results for Method 2 (Parallel Processing) after 1 runs:\n",
            "{'Method': 'Parallel Processing', 'Initial Size': 1000000.0, 'Update Size': 100000.0, 'Initial Load Time': 87.78988122940063, 'Initial Records': 1000000.0, 'Initial Avg CPU': 4.132183908045975, 'Initial Max CPU': 5.9, 'Initial Avg Memory': 4.539080459770116, 'Initial Max Memory': 7.0, 'Update Load Time': 33.65342926979065, 'Update Records': 100000.0, 'Update Avg CPU': 2.5939393939393938, 'Update Max CPU': 5.4, 'Update Avg Memory': 2.1151515151515157, 'Update Max Memory': 2.5, 'Final Records': 1100000.0, 'Total Time': 121.44331049919128}\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Row-by-row, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.61%\n",
            "Max CPU Usage: 3.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.10%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.59%\n",
            "Max CPU Usage: 3.10%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 2.03%\n",
            "Max Memory Usage: 2.30%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Average Results for Method 3 (Row-by-row) after 1 runs:\n",
            "{'Method': 'Row-by-row', 'Initial Size': 1000000.0, 'Update Size': 100000.0, 'Initial Load Time': 505.00277495384216, 'Initial Records': 1000000.0, 'Initial Avg CPU': 1.6097029702970245, 'Initial Max CPU': 3.3, 'Initial Avg Memory': 2.0998019801980323, 'Initial Max Memory': 2.1, 'Update Load Time': 73.70274567604065, 'Update Records': 100000.0, 'Update Avg CPU': 1.5876712328767115, 'Update Max CPU': 3.1, 'Update Avg Memory': 2.0260273972602745, 'Update Max Memory': 2.3, 'Final Records': 1100000.0, 'Total Time': 578.7055206298828}\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Bulk Pandas, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.51%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.10%\n",
            "Average Memory Usage: 2.10%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 2.10%\n",
            "Count after initial load: 1000000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.51%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 2.05%\n",
            "Max Memory Usage: 2.30%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Average Results for Method 4 (Bulk Pandas) after 1 runs:\n",
            "{'Method': 'Bulk Pandas', 'Initial Size': 1000000.0, 'Update Size': 100000.0, 'Initial Load Time': 201.68786907196045, 'Initial Records': 1000000.0, 'Initial Avg CPU': 1.5097435897435878, 'Initial Max CPU': 2.9, 'Initial Avg Memory': 2.1000000000000063, 'Initial Max Memory': 2.1, 'Update Load Time': 46.331674098968506, 'Update Records': 100000.0, 'Update Avg CPU': 1.511111111111111, 'Update Max CPU': 2.9, 'Update Avg Memory': 2.0511111111111116, 'Update Max Memory': 2.3, 'Final Records': 1100000.0, 'Total Time': 248.01954317092896}\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Streaming Chunks, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.43%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 2.10%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 2.10%\n",
            "Count after initial load: 1000000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.55%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 2.09%\n",
            "Max Memory Usage: 2.30%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Average Results for Method 5 (Streaming Chunks) after 1 runs:\n",
            "{'Method': 'Streaming Chunks', 'Initial Size': 1000000.0, 'Update Size': 100000.0, 'Initial Load Time': 229.24127173423767, 'Initial Records': 1000000.0, 'Initial Avg CPU': 1.4269406392694055, 'Initial Max CPU': 2.9, 'Initial Avg Memory': 2.100000000000008, 'Initial Max Memory': 2.1, 'Update Load Time': 49.042675495147705, 'Update Records': 100000.0, 'Update Avg CPU': 1.5489361702127664, 'Update Max CPU': 2.9, 'Update Avg Memory': 2.0872340425531903, 'Update Max Memory': 2.3, 'Final Records': 1100000.0, 'Total Time': 278.2839472293854}\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Dask, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.74 ms\n",
            "[########################################] | 100% Completed | 101.55 ms\n",
            "[########################################] | 100% Completed | 101.61 ms\n",
            "[########################################] | 100% Completed | 101.80 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.50%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 2.20%\n",
            "Max Memory Usage: 2.20%\n",
            "Min Memory Usage: 2.10%\n",
            "Count after initial load: 1000000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 102.05 ms\n",
            "[########################################] | 100% Completed | 102.01 ms\n",
            "[########################################] | 100% Completed | 102.14 ms\n",
            "[########################################] | 100% Completed | 101.99 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.57%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 2.25%\n",
            "Max Memory Usage: 2.40%\n",
            "Min Memory Usage: 2.20%\n",
            "Final count: 1100000 records\n",
            "\n",
            "Average Results for Method 6 (Dask) after 1 runs:\n",
            "{'Method': 'Dask', 'Initial Size': 1000000.0, 'Update Size': 100000.0, 'Initial Load Time': 50.59583258628845, 'Initial Records': 1000000.0, 'Initial Avg CPU': 1.5020408163265304, 'Initial Max CPU': 2.9, 'Initial Avg Memory': 2.1959183673469407, 'Initial Max Memory': 2.2, 'Update Load Time': 32.56592512130737, 'Update Records': 100000.0, 'Update Avg CPU': 1.5709677419354842, 'Update Max CPU': 2.9, 'Update Avg Memory': 2.245161290322581, 'Update Max Memory': 2.4, 'Final Records': 1100000.0, 'Total Time': 83.16175770759583}\n",
            "\n",
            "Results for Initial Size: 1000000, Update Size: 100000\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "MySQL LOAD DATA      42.98s               1000000         1.5%            2.1%                 29.18s               100000          1.9%            2.2%                 72.17s         \n",
            "Parallel Processing  87.79s               1000000         4.1%            7.0%                 33.65s               100000          2.6%            2.5%                 121.44s        \n",
            "Row-by-row           505.00s              1000000         1.6%            2.1%                 73.70s               100000          1.6%            2.3%                 578.71s        \n",
            "Bulk Pandas          201.69s              1000000         1.5%            2.1%                 46.33s               100000          1.5%            2.3%                 248.02s        \n",
            "Streaming Chunks     229.24s              1000000         1.4%            2.1%                 49.04s               100000          1.5%            2.3%                 278.28s        \n",
            "Dask                 50.60s               1000000         1.5%            2.2%                 32.57s               100000          1.6%            2.4%                 83.16s         \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 100000, Update Size: 10000\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (100000 records) ===\n",
            "Generated 100000 records for initial load\n",
            "\n",
            "=== 3. Creating Subsequent Load File (10000 records) ===\n",
            "Generated 10000 records for subsequent load\n",
            "\n",
            "=== Method 1: MySQL LOAD DATA ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Loading initial file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 100000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.50%\n",
            "Max CPU Usage: 2.30%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 10000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.77%\n",
            "Max CPU Usage: 2.20%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110000 records\n",
            "\n",
            "Average Results for Method 1 (MySQL LOAD DATA) after 1 runs:\n",
            "{'Method': 'MySQL LOAD DATA', 'Initial Size': 100000.0, 'Update Size': 10000.0, 'Initial Load Time': 3.776951551437378, 'Initial Records': 100000.0, 'Initial Avg CPU': 1.5, 'Initial Max CPU': 2.3, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 2.8997981548309326, 'Update Records': 10000.0, 'Update Avg CPU': 1.7666666666666666, 'Update Max CPU': 2.2, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110000.0, 'Total Time': 6.6767497062683105}\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Parallel Processing, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 25000 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 3.87%\n",
            "Max CPU Usage: 5.20%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 2.37%\n",
            "Max Memory Usage: 2.60%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 2500 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 10000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.97%\n",
            "Max CPU Usage: 2.50%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 2.05%\n",
            "Max Memory Usage: 2.20%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110000 records\n",
            "\n",
            "Average Results for Method 2 (Parallel Processing) after 1 runs:\n",
            "{'Method': 'Parallel Processing', 'Initial Size': 100000.0, 'Update Size': 10000.0, 'Initial Load Time': 8.444286584854126, 'Initial Records': 100000.0, 'Initial Avg CPU': 3.866666666666667, 'Initial Max CPU': 5.2, 'Initial Avg Memory': 2.366666666666667, 'Initial Max Memory': 2.6, 'Update Load Time': 3.5332751274108887, 'Update Records': 10000.0, 'Update Avg CPU': 1.9749999999999999, 'Update Max CPU': 2.5, 'Update Avg Memory': 2.05, 'Update Max Memory': 2.2, 'Final Records': 110000.0, 'Total Time': 11.977561712265015}\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Row-by-row, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.54%\n",
            "Max CPU Usage: 2.80%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.60%\n",
            "Max CPU Usage: 2.80%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110000 records\n",
            "\n",
            "Average Results for Method 3 (Row-by-row) after 1 runs:\n",
            "{'Method': 'Row-by-row', 'Initial Size': 100000.0, 'Update Size': 10000.0, 'Initial Load Time': 50.466752767562866, 'Initial Records': 100000.0, 'Initial Avg CPU': 1.5431372549019606, 'Initial Max CPU': 2.8, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 7.6398091316223145, 'Update Records': 10000.0, 'Update Avg CPU': 1.6, 'Update Max CPU': 2.8, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110000.0, 'Total Time': 58.10656189918518}\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Bulk Pandas, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.52%\n",
            "Max CPU Usage: 2.90%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.12%\n",
            "Max CPU Usage: 1.30%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110000 records\n",
            "\n",
            "Average Results for Method 4 (Bulk Pandas) after 1 runs:\n",
            "{'Method': 'Bulk Pandas', 'Initial Size': 100000.0, 'Update Size': 10000.0, 'Initial Load Time': 21.226390838623047, 'Initial Records': 100000.0, 'Initial Avg CPU': 1.519047619047619, 'Initial Max CPU': 2.9, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 4.552966356277466, 'Update Records': 10000.0, 'Update Avg CPU': 1.12, 'Update Max CPU': 1.3, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110000.0, 'Total Time': 25.779357194900513}\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Streaming Chunks, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.39%\n",
            "Max CPU Usage: 2.70%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.73%\n",
            "Max CPU Usage: 2.70%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110000 records\n",
            "\n",
            "Average Results for Method 5 (Streaming Chunks) after 1 runs:\n",
            "{'Method': 'Streaming Chunks', 'Initial Size': 100000.0, 'Update Size': 10000.0, 'Initial Load Time': 24.66809606552124, 'Initial Records': 100000.0, 'Initial Avg CPU': 1.3875000000000002, 'Initial Max CPU': 2.7, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 5.133131980895996, 'Update Records': 10000.0, 'Update Avg CPU': 1.7333333333333334, 'Update Max CPU': 2.7, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110000.0, 'Total Time': 29.801228046417236}\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Dask, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 102.19 ms\n",
            "[########################################] | 100% Completed | 102.14 ms\n",
            "[########################################] | 100% Completed | 101.94 ms\n",
            "[########################################] | 100% Completed | 102.07 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.06%\n",
            "Max CPU Usage: 1.50%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.96 ms\n",
            "[########################################] | 100% Completed | 101.82 ms\n",
            "[########################################] | 100% Completed | 101.78 ms\n",
            "[########################################] | 100% Completed | 101.73 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 2.00%\n",
            "Max CPU Usage: 2.80%\n",
            "Min CPU Usage: 1.30%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110000 records\n",
            "\n",
            "Average Results for Method 6 (Dask) after 1 runs:\n",
            "{'Method': 'Dask', 'Initial Size': 100000.0, 'Update Size': 10000.0, 'Initial Load Time': 6.734736442565918, 'Initial Records': 100000.0, 'Initial Avg CPU': 1.0571428571428572, 'Initial Max CPU': 1.5, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 3.564863681793213, 'Update Records': 10000.0, 'Update Avg CPU': 2.0, 'Update Max CPU': 2.8, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110000.0, 'Total Time': 10.29960012435913}\n",
            "\n",
            "Results for Initial Size: 100000, Update Size: 10000\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "MySQL LOAD DATA      3.78s                100000          1.5%            2.0%                 2.90s                10000           1.8%            2.0%                 6.68s          \n",
            "Parallel Processing  8.44s                100000          3.9%            2.6%                 3.53s                10000           2.0%            2.2%                 11.98s         \n",
            "Row-by-row           50.47s               100000          1.5%            2.0%                 7.64s                10000           1.6%            2.0%                 58.11s         \n",
            "Bulk Pandas          21.23s               100000          1.5%            2.0%                 4.55s                10000           1.1%            2.0%                 25.78s         \n",
            "Streaming Chunks     24.67s               100000          1.4%            2.0%                 5.13s                10000           1.7%            2.0%                 29.80s         \n",
            "Dask                 6.73s                100000          1.1%            2.0%                 3.56s                10000           2.0%            2.0%                 10.30s         \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 10000, Update Size: 1000\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (10000 records) ===\n",
            "Generated 10000 records for initial load\n",
            "\n",
            "=== 3. Creating Subsequent Load File (1000 records) ===\n",
            "Generated 1000 records for subsequent load\n",
            "\n",
            "=== Method 1: MySQL LOAD DATA ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Loading initial file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 10000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.70%\n",
            "Max CPU Usage: 0.70%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 10000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 1000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 11000 records\n",
            "\n",
            "Average Results for Method 1 (MySQL LOAD DATA) after 1 runs:\n",
            "{'Method': 'MySQL LOAD DATA', 'Initial Size': 10000.0, 'Update Size': 1000.0, 'Initial Load Time': 0.736370325088501, 'Initial Records': 10000.0, 'Initial Avg CPU': 0.7, 'Initial Max CPU': 0.7, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.35141944885253906, 'Update Records': 1000.0, 'Update Avg CPU': 0.6, 'Update Max CPU': 0.6, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 11000.0, 'Total Time': 1.08778977394104}\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Parallel Processing, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 2500 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 10000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.90%\n",
            "Max CPU Usage: 3.30%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.05%\n",
            "Max Memory Usage: 2.10%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 10000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 250 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 1000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 11000 records\n",
            "\n",
            "Average Results for Method 2 (Parallel Processing) after 1 runs:\n",
            "{'Method': 'Parallel Processing', 'Initial Size': 10000.0, 'Update Size': 1000.0, 'Initial Load Time': 1.3618381023406982, 'Initial Records': 10000.0, 'Initial Avg CPU': 1.9, 'Initial Max CPU': 3.3, 'Initial Avg Memory': 2.05, 'Initial Max Memory': 2.1, 'Update Load Time': 0.4846186637878418, 'Update Records': 1000.0, 'Update Avg CPU': 0.6, 'Update Max CPU': 0.6, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 11000.0, 'Total Time': 1.84645676612854}\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Row-by-row, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.70%\n",
            "Max CPU Usage: 2.80%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 10000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.00%\n",
            "Max CPU Usage: 1.00%\n",
            "Min CPU Usage: 1.00%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 11000 records\n",
            "\n",
            "Average Results for Method 3 (Row-by-row) after 1 runs:\n",
            "{'Method': 'Row-by-row', 'Initial Size': 10000.0, 'Update Size': 1000.0, 'Initial Load Time': 5.78523850440979, 'Initial Records': 10000.0, 'Initial Avg CPU': 1.7, 'Initial Max CPU': 2.8, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.8543384075164795, 'Update Records': 1000.0, 'Update Avg CPU': 1.0, 'Update Max CPU': 1.0, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 11000.0, 'Total Time': 6.6395769119262695}\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Bulk Pandas, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.00%\n",
            "Max CPU Usage: 1.10%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 10000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 11000 records\n",
            "\n",
            "Average Results for Method 4 (Bulk Pandas) after 1 runs:\n",
            "{'Method': 'Bulk Pandas', 'Initial Size': 10000.0, 'Update Size': 1000.0, 'Initial Load Time': 2.5525312423706055, 'Initial Records': 10000.0, 'Initial Avg CPU': 1.0, 'Initial Max CPU': 1.1, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.5953254699707031, 'Update Records': 1000.0, 'Update Avg CPU': 0.6, 'Update Max CPU': 0.6, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 11000.0, 'Total Time': 3.1478567123413086}\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Streaming Chunks, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.03%\n",
            "Max CPU Usage: 1.40%\n",
            "Min CPU Usage: 0.80%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 10000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 2.30%\n",
            "Max CPU Usage: 2.30%\n",
            "Min CPU Usage: 2.30%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 11000 records\n",
            "\n",
            "Average Results for Method 5 (Streaming Chunks) after 1 runs:\n",
            "{'Method': 'Streaming Chunks', 'Initial Size': 10000.0, 'Update Size': 1000.0, 'Initial Load Time': 2.8126139640808105, 'Initial Records': 10000.0, 'Initial Avg CPU': 1.0333333333333334, 'Initial Max CPU': 1.4, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.47643041610717773, 'Update Records': 1000.0, 'Update Avg CPU': 2.3, 'Update Max CPU': 2.3, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 11000.0, 'Total Time': 3.2890443801879883}\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Dask, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.97 ms\n",
            "[########################################] | 100% Completed | 101.89 ms\n",
            "[########################################] | 100% Completed | 101.70 ms\n",
            "[########################################] | 100% Completed | 102.06 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.40%\n",
            "Max CPU Usage: 1.90%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 10000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.62 ms\n",
            "[########################################] | 100% Completed | 101.36 ms\n",
            "[########################################] | 100% Completed | 101.37 ms\n",
            "[########################################] | 100% Completed | 101.33 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 11000 records\n",
            "\n",
            "Average Results for Method 6 (Dask) after 1 runs:\n",
            "{'Method': 'Dask', 'Initial Size': 10000.0, 'Update Size': 1000.0, 'Initial Load Time': 1.5446648597717285, 'Initial Records': 10000.0, 'Initial Avg CPU': 1.4, 'Initial Max CPU': 1.9, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.815561056137085, 'Update Records': 1000.0, 'Update Avg CPU': 0.6, 'Update Max CPU': 0.6, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 11000.0, 'Total Time': 2.3602259159088135}\n",
            "\n",
            "Results for Initial Size: 10000, Update Size: 1000\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "MySQL LOAD DATA      0.74s                10000           0.7%            2.0%                 0.35s                1000            0.6%            2.0%                 1.09s          \n",
            "Parallel Processing  1.36s                10000           1.9%            2.1%                 0.48s                1000            0.6%            2.0%                 1.85s          \n",
            "Row-by-row           5.79s                10000           1.7%            2.0%                 0.85s                1000            1.0%            2.0%                 6.64s          \n",
            "Bulk Pandas          2.55s                10000           1.0%            2.0%                 0.60s                1000            0.6%            2.0%                 3.15s          \n",
            "Streaming Chunks     2.81s                10000           1.0%            2.0%                 0.48s                1000            2.3%            2.0%                 3.29s          \n",
            "Dask                 1.54s                10000           1.4%            2.0%                 0.82s                1000            0.6%            2.0%                 2.36s          \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 1000, Update Size: 100\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (1000 records) ===\n",
            "Generated 1000 records for initial load\n",
            "\n",
            "=== 3. Creating Subsequent Load File (100 records) ===\n",
            "Generated 100 records for subsequent load\n",
            "\n",
            "=== Method 1: MySQL LOAD DATA ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Loading initial file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 1000 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 100 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100 records\n",
            "\n",
            "Average Results for Method 1 (MySQL LOAD DATA) after 1 runs:\n",
            "{'Method': 'MySQL LOAD DATA', 'Initial Size': 1000.0, 'Update Size': 100.0, 'Initial Load Time': 0.18523311614990234, 'Initial Records': 1000.0, 'Initial Avg CPU': 0.6, 'Initial Max CPU': 0.6, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.09080862998962402, 'Update Records': 100.0, 'Update Avg CPU': 0.3, 'Update Max CPU': 0.3, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 1100.0, 'Total Time': 0.27604174613952637}\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Parallel Processing, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 250 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 1000 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 25 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.90%\n",
            "Max CPU Usage: 0.90%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100 records\n",
            "\n",
            "Average Results for Method 2 (Parallel Processing) after 1 runs:\n",
            "{'Method': 'Parallel Processing', 'Initial Size': 1000.0, 'Update Size': 100.0, 'Initial Load Time': 0.3223602771759033, 'Initial Records': 1000.0, 'Initial Avg CPU': 0.3, 'Initial Max CPU': 0.3, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.2216651439666748, 'Update Records': 100.0, 'Update Avg CPU': 0.9, 'Update Max CPU': 0.9, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 1100.0, 'Total Time': 0.5440254211425781}\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Row-by-row, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.70%\n",
            "Max CPU Usage: 0.70%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100 records\n",
            "\n",
            "Average Results for Method 3 (Row-by-row) after 1 runs:\n",
            "{'Method': 'Row-by-row', 'Initial Size': 1000.0, 'Update Size': 100.0, 'Initial Load Time': 0.5528285503387451, 'Initial Records': 1000.0, 'Initial Avg CPU': 0.5, 'Initial Max CPU': 0.5, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.11681652069091797, 'Update Records': 100.0, 'Update Avg CPU': 0.7, 'Update Max CPU': 0.7, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 1100.0, 'Total Time': 0.6696450710296631}\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Bulk Pandas, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.90%\n",
            "Max CPU Usage: 1.90%\n",
            "Min CPU Usage: 1.90%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100 records\n",
            "\n",
            "Average Results for Method 4 (Bulk Pandas) after 1 runs:\n",
            "{'Method': 'Bulk Pandas', 'Initial Size': 1000.0, 'Update Size': 100.0, 'Initial Load Time': 0.3153865337371826, 'Initial Records': 1000.0, 'Initial Avg CPU': 0.4, 'Initial Max CPU': 0.4, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.09741830825805664, 'Update Records': 100.0, 'Update Avg CPU': 1.9, 'Update Max CPU': 1.9, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 1100.0, 'Total Time': 0.41280484199523926}\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Streaming Chunks, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.50%\n",
            "Max CPU Usage: 1.50%\n",
            "Min CPU Usage: 1.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.90%\n",
            "Max CPU Usage: 0.90%\n",
            "Min CPU Usage: 0.90%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100 records\n",
            "\n",
            "Average Results for Method 5 (Streaming Chunks) after 1 runs:\n",
            "{'Method': 'Streaming Chunks', 'Initial Size': 1000.0, 'Update Size': 100.0, 'Initial Load Time': 0.47240281105041504, 'Initial Records': 1000.0, 'Initial Avg CPU': 1.5, 'Initial Max CPU': 1.5, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.08790087699890137, 'Update Records': 100.0, 'Update Avg CPU': 0.9, 'Update Max CPU': 0.9, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 1100.0, 'Total Time': 0.5603036880493164}\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Dask, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.82 ms\n",
            "[########################################] | 100% Completed | 101.67 ms\n",
            "[########################################] | 100% Completed | 101.61 ms\n",
            "[########################################] | 100% Completed | 101.50 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 1000 records\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.78 ms\n",
            "[########################################] | 100% Completed | 101.31 ms\n",
            "[########################################] | 100% Completed | 101.51 ms\n",
            "[########################################] | 100% Completed | 101.35 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 1100 records\n",
            "\n",
            "Average Results for Method 6 (Dask) after 1 runs:\n",
            "{'Method': 'Dask', 'Initial Size': 1000.0, 'Update Size': 100.0, 'Initial Load Time': 0.6958858966827393, 'Initial Records': 1000.0, 'Initial Avg CPU': 0.4, 'Initial Max CPU': 0.4, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.5805633068084717, 'Update Records': 100.0, 'Update Avg CPU': 0.5, 'Update Max CPU': 0.5, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 1100.0, 'Total Time': 1.276449203491211}\n",
            "\n",
            "Results for Initial Size: 1000, Update Size: 100\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "MySQL LOAD DATA      0.19s                1000            0.6%            2.0%                 0.09s                100             0.3%            2.0%                 0.28s          \n",
            "Parallel Processing  0.32s                1000            0.3%            2.0%                 0.22s                100             0.9%            2.0%                 0.54s          \n",
            "Row-by-row           0.55s                1000            0.5%            2.0%                 0.12s                100             0.7%            2.0%                 0.67s          \n",
            "Bulk Pandas          0.32s                1000            0.4%            2.0%                 0.10s                100             1.9%            2.0%                 0.41s          \n",
            "Streaming Chunks     0.47s                1000            1.5%            2.0%                 0.09s                100             0.9%            2.0%                 0.56s          \n",
            "Dask                 0.70s                1000            0.4%            2.0%                 0.58s                100             0.5%            2.0%                 1.28s          \n",
            "\n",
            "\n",
            "================================================\n",
            "Testing with Initial Size: 100, Update Size: 10\n",
            "================================================\n",
            "\n",
            "Resetting table for new data size test...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "=== 2. Creating Initial Load File (100 records) ===\n",
            "Generated 100 records for initial load\n",
            "\n",
            "=== 3. Creating Subsequent Load File (10 records) ===\n",
            "Generated 10 records for subsequent load\n",
            "\n",
            "=== Method 1: MySQL LOAD DATA ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Loading initial file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 100 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.40%\n",
            "Max CPU Usage: 0.40%\n",
            "Min CPU Usage: 0.40%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100 records\n",
            "\n",
            "Loading subsequent file...\n",
            "MySQL secure_file_priv: /content/mysql/\n",
            "Trying LOAD DATA LOCAL INFILE...\n",
            "LOCAL INFILE failed: LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.\n",
            "Trying standard LOAD DATA INFILE...\n",
            "LOAD DATA INFILE operation successful, loaded 10 records\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110 records\n",
            "\n",
            "Average Results for Method 1 (MySQL LOAD DATA) after 1 runs:\n",
            "{'Method': 'MySQL LOAD DATA', 'Initial Size': 100.0, 'Update Size': 10.0, 'Initial Load Time': 0.05318713188171387, 'Initial Records': 100.0, 'Initial Avg CPU': 0.4, 'Initial Max CPU': 0.4, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.06838035583496094, 'Update Records': 10.0, 'Update Avg CPU': 0.2, 'Update Max CPU': 0.2, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110.0, 'Total Time': 0.1215674877166748}\n",
            "\n",
            "=== Method 2: Parallel Processing ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Parallel Processing, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "Split data into 4 chunks of approximately 25 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 100 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100 records\n",
            "\n",
            "Loading subsequent file...\n",
            "Split data into 4 chunks of approximately 2 records each\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 10 records in parallel\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110 records\n",
            "\n",
            "Average Results for Method 2 (Parallel Processing) after 1 runs:\n",
            "{'Method': 'Parallel Processing', 'Initial Size': 100.0, 'Update Size': 10.0, 'Initial Load Time': 0.17844247817993164, 'Initial Records': 100.0, 'Initial Avg CPU': 0.3, 'Initial Max CPU': 0.3, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.204209566116333, 'Update Records': 10.0, 'Update Avg CPU': 0.5, 'Update Max CPU': 0.5, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110.0, 'Total Time': 0.38265204429626465}\n",
            "\n",
            "=== Method 3: Row-by-row ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Row-by-row, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110 records\n",
            "\n",
            "Average Results for Method 3 (Row-by-row) after 1 runs:\n",
            "{'Method': 'Row-by-row', 'Initial Size': 100.0, 'Update Size': 10.0, 'Initial Load Time': 0.08139538764953613, 'Initial Records': 100.0, 'Initial Avg CPU': 0.5, 'Initial Max CPU': 0.5, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.06112074851989746, 'Update Records': 10.0, 'Update Avg CPU': 0.2, 'Update Max CPU': 0.2, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110.0, 'Total Time': 0.1425161361694336}\n",
            "\n",
            "=== Method 4: Bulk Pandas ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Bulk Pandas, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.50%\n",
            "Max CPU Usage: 1.50%\n",
            "Min CPU Usage: 1.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.80%\n",
            "Max CPU Usage: 1.80%\n",
            "Min CPU Usage: 1.80%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110 records\n",
            "\n",
            "Average Results for Method 4 (Bulk Pandas) after 1 runs:\n",
            "{'Method': 'Bulk Pandas', 'Initial Size': 100.0, 'Update Size': 10.0, 'Initial Load Time': 0.0525209903717041, 'Initial Records': 100.0, 'Initial Avg CPU': 1.5, 'Initial Max CPU': 1.5, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.056374311447143555, 'Update Records': 10.0, 'Update Avg CPU': 1.8, 'Update Max CPU': 1.8, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110.0, 'Total Time': 0.10889530181884766}\n",
            "\n",
            "=== Method 5: Streaming Chunks ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Streaming Chunks, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100 records\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110 records\n",
            "\n",
            "Average Results for Method 5 (Streaming Chunks) after 1 runs:\n",
            "{'Method': 'Streaming Chunks', 'Initial Size': 100.0, 'Update Size': 10.0, 'Initial Load Time': 0.050367116928100586, 'Initial Records': 100.0, 'Initial Avg CPU': 0.5, 'Initial Max CPU': 0.5, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.05103135108947754, 'Update Records': 10.0, 'Update Avg CPU': 0.2, 'Update Max CPU': 0.2, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110.0, 'Total Time': 0.10139846801757812}\n",
            "\n",
            "=== Method 6: Dask ===\n",
            "\n",
            "Run 1 of 1\n",
            "\n",
            "Resetting table for method Dask, run 1...\n",
            "Table dropped and recreated successfully\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.79 ms\n",
            "[########################################] | 100% Completed | 101.54 ms\n",
            "[########################################] | 100% Completed | 101.39 ms\n",
            "[########################################] | 100% Completed | 101.38 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Count after initial load: 100 records\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.44 ms\n",
            "[########################################] | 100% Completed | 101.24 ms\n",
            "[########################################] | 100% Completed | 101.22 ms\n",
            "[########################################] | 100% Completed | 101.32 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 2.00%\n",
            "Max Memory Usage: 2.00%\n",
            "Min Memory Usage: 2.00%\n",
            "Final count: 110 records\n",
            "\n",
            "Average Results for Method 6 (Dask) after 1 runs:\n",
            "{'Method': 'Dask', 'Initial Size': 100.0, 'Update Size': 10.0, 'Initial Load Time': 0.5436766147613525, 'Initial Records': 100.0, 'Initial Avg CPU': 0.2, 'Initial Max CPU': 0.2, 'Initial Avg Memory': 2.0, 'Initial Max Memory': 2.0, 'Update Load Time': 0.5384683609008789, 'Update Records': 10.0, 'Update Avg CPU': 0.3, 'Update Max CPU': 0.3, 'Update Avg Memory': 2.0, 'Update Max Memory': 2.0, 'Final Records': 110.0, 'Total Time': 1.0821449756622314}\n",
            "\n",
            "Results for Initial Size: 100, Update Size: 10\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "MySQL LOAD DATA      0.05s                100             0.4%            2.0%                 0.07s                10              0.2%            2.0%                 0.12s          \n",
            "Parallel Processing  0.18s                100             0.3%            2.0%                 0.20s                10              0.5%            2.0%                 0.38s          \n",
            "Row-by-row           0.08s                100             0.5%            2.0%                 0.06s                10              0.2%            2.0%                 0.14s          \n",
            "Bulk Pandas          0.05s                100             1.5%            2.0%                 0.06s                10              1.8%            2.0%                 0.11s          \n",
            "Streaming Chunks     0.05s                100             0.5%            2.0%                 0.05s                10              0.2%            2.0%                 0.10s          \n",
            "Dask                 0.54s                100             0.2%            2.0%                 0.54s                10              0.3%            2.0%                 1.08s          \n",
            "\n",
            "\n",
            "Comparative Summary Across All Data Sizes\n",
            "====================================================================================================\n",
            "Data Size (Initial, Update) | Best Method | Worst Method | Average Load Time\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}