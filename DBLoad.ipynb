{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# System-level dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y postgresql postgresql-contrib python3-dev libpq-dev\n",
        "\n",
        "# Python packages\n",
        "!pip install pandas numpy sqlalchemy psutil mimesis dask \"dask[dataframe]\" tqdm psycopg2-binary"
      ],
      "metadata": {
        "id": "QS2fUsNN4WRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdbe97a-ab46-4b0d-9d86-5cd48d276b98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libpq-dev is already the newest version (14.15-0ubuntu0.22.04.1).\n",
            "libpq-dev set to manually installed.\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "python3-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl logrotate\n",
            "  postgresql-14 postgresql-client-14 postgresql-client-common postgresql-common ssl-cert sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl logrotate postgresql\n",
            "  postgresql-14 postgresql-client-14 postgresql-client-common postgresql-common postgresql-contrib\n",
            "  ssl-cert sysstat\n",
            "0 upgraded, 13 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 18.4 MB of archives.\n",
            "After this operation, 51.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-xs-perl amd64 4.030-1build3 [87.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.15-0ubuntu0.22.04.1 [1,225 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.15-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 18.4 MB in 2s (10.8 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 121634 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../01-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../02-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../03-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../04-libjson-xs-perl_4.030-1build3_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.030-1build3) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../05-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../06-postgresql-client-14_14.15-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.15-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../07-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../08-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../09-postgresql-14_14.15-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.15-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../10-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package postgresql-contrib.\n",
            "Preparing to unpack .../11-postgresql-contrib_14+238_all.deb ...\n",
            "Unpacking postgresql-contrib (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../12-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up postgresql-client-14 (14.15-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n",
            "Setting up libjson-xs-perl (4.030-1build3) ...\n",
            "Setting up postgresql-common (238) ...\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.15-0ubuntu0.22.04.1) ...\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-contrib (14+238) ...\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.38)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: mimesis in /usr/local/lib/python3.11/dist-packages (18.0.0)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2025.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.12.2)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.0.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (19.0.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "from typing import List, Dict\n",
        "from sqlalchemy import create_engine, text\n",
        "from multiprocessing import Pool\n",
        "import dask.dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "from mimesis.locales import Locale\n",
        "from mimesis.schema import Fieldset\n",
        "import tempfile\n",
        "import io\n",
        "\n",
        "# Docker setup for PostgreSQL\n",
        "def setup_postgres_colab():\n",
        "    \"\"\"Setup PostgreSQL in Google Colab\"\"\"\n",
        "    print(\"Setting up PostgreSQL in Google Colab...\")\n",
        "\n",
        "    # Install PostgreSQL\n",
        "    !apt-get update\n",
        "    !apt-get install -y postgresql postgresql-contrib\n",
        "\n",
        "    # Start PostgreSQL service\n",
        "    !service postgresql start\n",
        "\n",
        "    # Configure PostgreSQL to accept connections\n",
        "    !sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'password';\"\n",
        "    !sudo -u postgres psql -c \"CREATE DATABASE employees;\"\n",
        "\n",
        "    # Update pg_hba.conf to allow local connections\n",
        "    !echo \"host all all 0.0.0.0/0 md5\" | sudo tee -a /etc/postgresql/*/main/pg_hba.conf\n",
        "\n",
        "    # Update postgresql.conf to listen on all addresses\n",
        "    !echo \"listen_addresses = '*'\" | sudo tee -a /etc/postgresql/*/main/postgresql.conf\n",
        "\n",
        "    # Restart PostgreSQL to apply changes\n",
        "    !service postgresql restart\n",
        "\n",
        "    # Wait for PostgreSQL to be ready\n",
        "    connection_string = \"postgresql://postgres:password@localhost:5432/employees\"\n",
        "    engine = create_engine(connection_string)\n",
        "\n",
        "    max_attempts = 30\n",
        "    attempt = 0\n",
        "    while attempt < max_attempts:\n",
        "        try:\n",
        "            print(f\"Attempting to connect to database... (Attempt {attempt + 1}/{max_attempts})\")\n",
        "            with engine.connect() as connection:\n",
        "                connection.execute(text(\"SELECT 1\"))\n",
        "            print(\"Successfully connected to PostgreSQL!\")\n",
        "            return connection_string\n",
        "        except Exception as e:\n",
        "            print(f\"Connection attempt failed: {str(e)}\")\n",
        "            attempt += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "    raise Exception(\"Failed to connect to PostgreSQL after maximum attempts\")\n",
        "\n",
        "# Database schema setup\n",
        "def setup_database(engine):\n",
        "    \"\"\"Create SCD Type 2 table schema\"\"\"\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS employees (\n",
        "                employee_id INTEGER,\n",
        "                name VARCHAR(100),\n",
        "                email VARCHAR(100),\n",
        "                address TEXT,\n",
        "                phone VARCHAR(50),\n",
        "                date_of_birth DATE,\n",
        "                gender VARCHAR(10),\n",
        "                company VARCHAR(100),\n",
        "                position VARCHAR(100),\n",
        "                salary DECIMAL(10,2),\n",
        "                retired VARCHAR(3),\n",
        "                valid_from TIMESTAMP,\n",
        "                valid_to TIMESTAMP,\n",
        "                is_current BOOLEAN,\n",
        "                PRIMARY KEY (employee_id, valid_from)\n",
        "            )\n",
        "        \"\"\"))\n",
        "        conn.commit()\n",
        "\n",
        "def generate_data(row_count: int) -> pd.DataFrame:\n",
        "    \"\"\"Generate synthetic data using Mimesis Fieldset\"\"\"\n",
        "    fieldset = Fieldset(locale=Locale.EN)\n",
        "\n",
        "    # Generate all fields at once using Fieldset\n",
        "    employee_ids = list(range(row_count))\n",
        "    names = fieldset(\"full_name\", i=row_count)\n",
        "    emails = fieldset(\"email\", i=row_count)\n",
        "    addresses = fieldset(\"address\", i=row_count)\n",
        "    phones = fieldset(\"telephone\", i=row_count)\n",
        "    dates = [str(date.isoformat()) for date in fieldset(\"date\", start=1950, end=2005, i=row_count)]\n",
        "    genders = np.random.choice([\"Male\", \"Female\"], size=row_count).tolist()\n",
        "    cities = fieldset(\"city\", i=row_count)\n",
        "    positions = fieldset(\"occupation\", i=row_count)\n",
        "    salaries = np.round(np.random.uniform(30000, 200000, row_count), 2).tolist()\n",
        "    retired = np.random.choice([\"Yes\", \"No\"], size=row_count).tolist()\n",
        "\n",
        "    # Create records using list comprehension with zip\n",
        "    records = [\n",
        "        {\n",
        "            \"employee_id\": emp_id,\n",
        "            \"name\": name,\n",
        "            \"email\": email,\n",
        "            \"address\": address,\n",
        "            \"phone\": phone,\n",
        "            \"date_of_birth\": dob,\n",
        "            \"gender\": gender,\n",
        "            \"company\": f\"{city} Corp\",\n",
        "            \"position\": position,\n",
        "            \"salary\": salary,\n",
        "            \"retired\": retired_status\n",
        "        }\n",
        "        for emp_id, name, email, address, phone, dob, gender, city, position, salary, retired_status\n",
        "        in zip(employee_ids, names, emails, addresses, phones, dates, genders,\n",
        "               cities, positions, salaries, retired)\n",
        "    ]\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def identify_changes(new_df: pd.DataFrame, engine) -> pd.DataFrame:\n",
        "    \"\"\"Compare new data with existing records and identify changes\"\"\"\n",
        "    current_records = pd.read_sql(\n",
        "        \"\"\"\n",
        "        SELECT * FROM employees\n",
        "        WHERE is_current = true\n",
        "        \"\"\",\n",
        "        engine\n",
        "    )\n",
        "\n",
        "    if len(current_records) > 0:\n",
        "        merged = new_df.merge(\n",
        "            current_records,\n",
        "            on='employee_id',\n",
        "            how='left',\n",
        "            suffixes=('_new', '_current')\n",
        "        )\n",
        "\n",
        "        changed_mask = (\n",
        "            (merged['name_new'] != merged['name_current']) |\n",
        "            (merged['email_new'] != merged['email_current']) |\n",
        "            (merged['address_new'] != merged['address_current']) |\n",
        "            (merged['phone_new'] != merged['phone_current']) |\n",
        "            (merged['position_new'] != merged['position_current']) |\n",
        "            (merged['salary_new'] != merged['salary_current'])\n",
        "        )\n",
        "\n",
        "        new_mask = merged['name_current'].isna()\n",
        "\n",
        "        new_df['change_type'] = 'no_change'\n",
        "        new_df.loc[new_mask, 'change_type'] = 'insert'\n",
        "        new_df.loc[changed_mask & ~new_mask, 'change_type'] = 'update'\n",
        "    else:\n",
        "        new_df['change_type'] = 'insert'\n",
        "\n",
        "    return new_df\n",
        "\n",
        "def apply_scd2_changes(df: pd.DataFrame, engine) -> pd.DataFrame:\n",
        "    \"\"\"Apply SCD Type 2 changes to the data\"\"\"\n",
        "    current_timestamp = datetime.now().isoformat()\n",
        "\n",
        "    df['valid_from'] = current_timestamp\n",
        "    df['valid_to'] = '9999-12-31 23:59:59'\n",
        "    df['is_current'] = True\n",
        "\n",
        "    updates = df[df['change_type'] == 'update']\n",
        "    if not updates.empty:\n",
        "        with engine.begin() as conn:\n",
        "            employee_ids = tuple(updates['employee_id'].tolist())\n",
        "            conn.execute(\n",
        "                text(\"\"\"\n",
        "                    UPDATE employees\n",
        "                    SET valid_to = :valid_to,\n",
        "                        is_current = FALSE\n",
        "                    WHERE employee_id IN :employee_ids\n",
        "                    AND is_current = TRUE\n",
        "                \"\"\"),\n",
        "                {\n",
        "                    \"valid_to\": current_timestamp,\n",
        "                    \"employee_ids\": employee_ids\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return df.drop(columns=['change_type'])\n",
        "\n",
        "def truncate_table(engine):\n",
        "    \"\"\"Truncate the employees table\"\"\"\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(\"TRUNCATE TABLE employees\"))\n",
        "        conn.commit()\n",
        "    print(\"Table truncated successfully\")\n",
        "\n",
        "# Resource monitoring\n",
        "def monitor_resources(interval, stats):\n",
        "    \"\"\"Monitor CPU and memory usage\"\"\"\n",
        "    while not stats['stop']:\n",
        "        stats['cpu'].append(psutil.cpu_percent(interval=None))\n",
        "        stats['memory'].append(psutil.virtual_memory().percent)\n",
        "        time.sleep(interval)\n",
        "\n",
        "def print_resource_stats(stats):\n",
        "    \"\"\"Print resource usage statistics\"\"\"\n",
        "    print(\"\\nResource Usage Statistics:\")\n",
        "    print(f\"Average CPU Usage: {sum(stats['cpu']) / len(stats['cpu']):.2f}%\")\n",
        "    print(f\"Max CPU Usage: {max(stats['cpu']):.2f}%\")\n",
        "    print(f\"Min CPU Usage: {min(stats['cpu']):.2f}%\")\n",
        "    print(f\"Average Memory Usage: {sum(stats['memory']) / len(stats['memory']):.2f}%\")\n",
        "    print(f\"Max Memory Usage: {max(stats['memory']):.2f}%\")\n",
        "    print(f\"Min Memory Usage: {min(stats['memory']):.2f}%\")\n",
        "\n",
        "def monitor_performance(func):\n",
        "    \"\"\"Decorator to monitor performance of loading methods\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        stats = {'cpu': [], 'memory': [], 'stop': False}\n",
        "\n",
        "        # Start monitoring thread\n",
        "        monitor_thread = threading.Thread(target=monitor_resources, args=(1, stats))\n",
        "        monitor_thread.start()\n",
        "\n",
        "        try:\n",
        "            # Execute the loading function\n",
        "            start_time = time.time()\n",
        "            func(*args, **kwargs)\n",
        "            duration = time.time() - start_time\n",
        "\n",
        "            # Stop monitoring\n",
        "            stats['stop'] = True\n",
        "            monitor_thread.join()\n",
        "\n",
        "            # Calculate resource statistics\n",
        "            resource_stats = {\n",
        "                'duration': duration,\n",
        "                'avg_cpu': sum(stats['cpu']) / len(stats['cpu']) if stats['cpu'] else 0,\n",
        "                'max_cpu': max(stats['cpu']) if stats['cpu'] else 0,\n",
        "                'avg_memory': sum(stats['memory']) / len(stats['memory']) if stats['memory'] else 0,\n",
        "                'max_memory': max(stats['memory']) if stats['memory'] else 0\n",
        "            }\n",
        "\n",
        "            print_resource_stats(stats)\n",
        "            return duration, resource_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            stats['stop'] = True\n",
        "            monitor_thread.join()\n",
        "            raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_row_by_row(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data row by row with SCD Type 2\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        for _, row in df.iterrows():\n",
        "            conn.execute(\n",
        "                text(\"\"\"\n",
        "                    INSERT INTO employees\n",
        "                    VALUES (:employee_id, :name, :email, :address, :phone,\n",
        "                           :date_of_birth, :gender, :company, :position,\n",
        "                           :salary, :retired, :valid_from, :valid_to, :is_current)\n",
        "                \"\"\"),\n",
        "                row.to_dict()\n",
        "            )\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_bulk_pandas(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data using pandas bulk insert\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "    df.to_sql('employees', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_streaming_chunks(df: pd.DataFrame, engine, chunk_size=1000):\n",
        "    \"\"\"Load data in chunks\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "\n",
        "    for chunk_start in range(0, len(df), chunk_size):\n",
        "        chunk = df.iloc[chunk_start:chunk_start + chunk_size]\n",
        "        chunk.to_sql('employees', engine, if_exists='append', index=False, method='multi')\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "def parallel_worker(chunk_data):\n",
        "    \"\"\"Worker function for parallel processing\"\"\"\n",
        "    engine = create_engine(\"postgresql://postgres:password@localhost:5432/postgres\")\n",
        "    chunk_data.to_sql('employees', engine, if_exists='append', index=False, method='multi')\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_parallel(df: pd.DataFrame, engine, num_processes=4):\n",
        "    \"\"\"Load data using parallel processing\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "\n",
        "    chunks = np.array_split(df, num_processes)\n",
        "    with Pool(num_processes) as pool:\n",
        "        pool.map(parallel_worker, chunks)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_dask(df: pd.DataFrame, engine, npartitions=4):\n",
        "    \"\"\"Load data using Dask\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "\n",
        "    ddf = dd.from_pandas(df, npartitions=npartitions)\n",
        "    with ProgressBar():\n",
        "        for partition in ddf.partitions:\n",
        "            partition.compute().to_sql('employees', engine, if_exists='append', index=False)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "# Update loading methods with performance monitoring\n",
        "@monitor_performance\n",
        "def load_postgres_copy(df: pd.DataFrame, engine):\n",
        "    \"\"\"Load data using PostgreSQL COPY command\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = identify_changes(df, engine)\n",
        "    df = apply_scd2_changes(df, engine)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
        "        df.to_csv(f, index=False, header=False, sep='\\t')\n",
        "        temp_file_path = f.name\n",
        "\n",
        "    try:\n",
        "        with engine.connect().execution_options(autocommit=True) as conn:\n",
        "            with open(temp_file_path, 'r') as f:\n",
        "                cursor = conn.connection.cursor()\n",
        "                cursor.copy_from(\n",
        "                    f,\n",
        "                    'employees',\n",
        "                    sep='\\t',\n",
        "                    columns=df.columns.tolist()\n",
        "                )\n",
        "    finally:\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return duration\n",
        "\n",
        "def main():\n",
        "    # 1. Setup Database\n",
        "    print(\"\\n=== 1. Setting up PostgreSQL Database ===\")\n",
        "    connection_string = setup_postgres_colab()\n",
        "    engine = create_engine(connection_string)\n",
        "    setup_database(engine)\n",
        "\n",
        "    # Rest of the function remains the same...\n",
        "\n",
        "    # 2. Create Initial Load File\n",
        "    print(\"\\n=== 2. Creating Initial Load File ===\")\n",
        "    initial_df = generate_data(100)\n",
        "    print(f\"Generated {len(initial_df)} records for initial load\")\n",
        "\n",
        "    # 3. Create Subsequent Load File\n",
        "    print(\"\\n=== 3. Creating Subsequent Load File ===\")\n",
        "    update_df = generate_data(50)\n",
        "    print(f\"Generated {len(update_df)} records for subsequent load\")\n",
        "\n",
        "    # Define the loading methods\n",
        "    methods = [\n",
        "        (load_row_by_row, \"Row-by-row\"),\n",
        "        (load_bulk_pandas, \"Bulk Pandas\"),\n",
        "        (load_streaming_chunks, \"Streaming Chunks\"),\n",
        "        (load_parallel, \"Parallel Processing\"),\n",
        "        (load_dask, \"Dask\"),\n",
        "        (load_postgres_copy, \"PostgreSQL COPY\")\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Execute methods sequentially\n",
        "    for idx, (method, name) in enumerate(methods, start=1):\n",
        "        print(f\"\\n=== Method {idx}: {name} ===\")\n",
        "\n",
        "        try:\n",
        "            # 4. Load Initial File\n",
        "            print(f\"\\nLoading initial file...\")\n",
        "            initial_duration, initial_stats = method(initial_df, engine)\n",
        "\n",
        "            # Get count after initial load\n",
        "            with engine.connect() as conn:\n",
        "                initial_count = conn.execute(text(\"SELECT COUNT(*) FROM employees\")).scalar()\n",
        "\n",
        "            # 5. Load Subsequent File\n",
        "            print(f\"\\nLoading subsequent file...\")\n",
        "            update_duration, update_stats = method(update_df, engine)\n",
        "\n",
        "            # Get final count\n",
        "            with engine.connect() as conn:\n",
        "                final_count = conn.execute(text(\"SELECT COUNT(*) FROM employees\")).scalar()\n",
        "\n",
        "            results.append({\n",
        "                'Method': name,\n",
        "                'Initial Load Time': f\"{initial_duration:.2f}s\",\n",
        "                'Initial Records': initial_count,\n",
        "                'Initial Avg CPU': f\"{initial_stats['avg_cpu']:.1f}%\",\n",
        "                'Initial Max CPU': f\"{initial_stats['max_cpu']:.1f}%\",\n",
        "                'Initial Avg Memory': f\"{initial_stats['avg_memory']:.1f}%\",\n",
        "                'Initial Max Memory': f\"{initial_stats['max_memory']:.1f}%\",\n",
        "                'Update Load Time': f\"{update_duration:.2f}s\",\n",
        "                'Update Records': final_count - initial_count,\n",
        "                'Update Avg CPU': f\"{update_stats['avg_cpu']:.1f}%\",\n",
        "                'Update Max CPU': f\"{update_stats['max_cpu']:.1f}%\",\n",
        "                'Update Avg Memory': f\"{update_stats['avg_memory']:.1f}%\",\n",
        "                'Update Max Memory': f\"{update_stats['max_memory']:.1f}%\",\n",
        "                'Final Records': final_count,\n",
        "                'Total Time': f\"{(initial_duration + update_duration):.2f}s\"\n",
        "            })\n",
        "\n",
        "            print(f\"\\nMethod {idx} Results:\")\n",
        "            print(f\"Initial Load: {initial_duration:.2f}s ({initial_count} records)\")\n",
        "            print(f\"Initial Load Resource Usage:\")\n",
        "            print(f\"  Avg CPU: {initial_stats['avg_cpu']:.1f}%, Max CPU: {initial_stats['max_cpu']:.1f}%\")\n",
        "            print(f\"  Avg Memory: {initial_stats['avg_memory']:.1f}%, Max Memory: {initial_stats['max_memory']:.1f}%\")\n",
        "            print(f\"\\nUpdate Load: {update_duration:.2f}s ({final_count - initial_count} records)\")\n",
        "            print(f\"Update Load Resource Usage:\")\n",
        "            print(f\"  Avg CPU: {update_stats['avg_cpu']:.1f}%, Max CPU: {update_stats['max_cpu']:.1f}%\")\n",
        "            print(f\"  Avg Memory: {update_stats['avg_memory']:.1f}%, Max Memory: {update_stats['max_memory']:.1f}%\")\n",
        "\n",
        "            # 6. Truncate table (if not the last method)\n",
        "            if idx < len(methods):\n",
        "                print(f\"\\nTruncating table for next method...\")\n",
        "                truncate_table(engine)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in {name} method: {str(e)}\")\n",
        "            results.append({\n",
        "                'Method': name,\n",
        "                'Initial Load Time': 'Failed',\n",
        "                'Initial Records': 'Failed',\n",
        "                'Initial Avg CPU': 'Failed',\n",
        "                'Initial Max CPU': 'Failed',\n",
        "                'Initial Avg Memory': 'Failed',\n",
        "                'Initial Max Memory': 'Failed',\n",
        "                'Update Load Time': 'Failed',\n",
        "                'Update Records': 'Failed',\n",
        "                'Update Avg CPU': 'Failed',\n",
        "                'Update Max CPU': 'Failed',\n",
        "                'Update Avg Memory': 'Failed',\n",
        "                'Update Max Memory': 'Failed',\n",
        "                'Final Records': 'Failed',\n",
        "                'Total Time': 'Failed'\n",
        "            })\n",
        "\n",
        "    # Print final results\n",
        "    print(\"\\nFinal Results:\")\n",
        "    print(\"=\" * 140)\n",
        "    headers = [\n",
        "        'Method', 'Initial Load Time', 'Initial Records', 'Initial Avg CPU', 'Initial Max Memory',\n",
        "        'Update Load Time', 'Update Records', 'Update Avg CPU', 'Update Max Memory',\n",
        "        'Total Time'\n",
        "    ]\n",
        "    row_format = \"{:<20} {:<20} {:<15} {:<15} {:<20} {:<20} {:<15} {:<15} {:<20} {:<15}\"\n",
        "    print(row_format.format(*headers))\n",
        "    print(\"-\" * 140)\n",
        "    for result in results:\n",
        "        print(row_format.format(\n",
        "            result['Method'],\n",
        "            result['Initial Load Time'],\n",
        "            str(result['Initial Records']),\n",
        "            result['Initial Avg CPU'],\n",
        "            result['Initial Max Memory'],\n",
        "            result['Update Load Time'],\n",
        "            str(result['Update Records']),\n",
        "            result['Update Avg CPU'],\n",
        "            result['Update Max Memory'],\n",
        "            result['Total Time']\n",
        "        ))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_0tIFbdSxnR",
        "outputId": "23689888-dbd0-49ad-a78c-3c2580446b8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 1. Setting up PostgreSQL Database ===\n",
            "Setting up PostgreSQL in Google Colab...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "postgresql is already the newest version (14+238).\n",
            "postgresql-contrib is already the newest version (14+238).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "ALTER ROLE\n",
            "ERROR:  database \"employees\" already exists\n",
            "host all all 0.0.0.0/0 md5\n",
            "listen_addresses = '*'\n",
            " * Restarting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "Attempting to connect to database... (Attempt 1/30)\n",
            "Successfully connected to PostgreSQL!\n",
            "\n",
            "=== 2. Creating Initial Load File ===\n",
            "Generated 100 records for initial load\n",
            "\n",
            "=== 3. Creating Subsequent Load File ===\n",
            "Generated 50 records for subsequent load\n",
            "\n",
            "=== Method 1: Row-by-row ===\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.70%\n",
            "Max CPU Usage: 0.70%\n",
            "Min CPU Usage: 0.70%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Method 1 Results:\n",
            "Initial Load: 0.06s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.7%, Max CPU: 0.7%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 0.04s (50 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.6%, Max CPU: 0.6%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Truncating table for next method...\n",
            "Table truncated successfully\n",
            "\n",
            "=== Method 2: Bulk Pandas ===\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 2.00%\n",
            "Max CPU Usage: 2.00%\n",
            "Min CPU Usage: 2.00%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 1.10%\n",
            "Max CPU Usage: 1.10%\n",
            "Min CPU Usage: 1.10%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Method 2 Results:\n",
            "Initial Load: 0.04s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 2.0%, Max CPU: 2.0%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 0.03s (50 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 1.1%, Max CPU: 1.1%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Truncating table for next method...\n",
            "Table truncated successfully\n",
            "\n",
            "=== Method 3: Streaming Chunks ===\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Method 3 Results:\n",
            "Initial Load: 0.03s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.5%, Max CPU: 0.5%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 0.03s (50 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Truncating table for next method...\n",
            "Table truncated successfully\n",
            "\n",
            "=== Method 4: Parallel Processing ===\n",
            "\n",
            "Loading initial file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Loading subsequent file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.60%\n",
            "Max CPU Usage: 0.60%\n",
            "Min CPU Usage: 0.60%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Method 4 Results:\n",
            "Initial Load: 0.17s (0 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 0.16s (0 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.6%, Max CPU: 0.6%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Truncating table for next method...\n",
            "Table truncated successfully\n",
            "\n",
            "=== Method 5: Dask ===\n",
            "\n",
            "Loading initial file...\n",
            "[########################################] | 100% Completed | 101.79 ms\n",
            "[########################################] | 100% Completed | 101.35 ms\n",
            "[########################################] | 100% Completed | 101.49 ms\n",
            "[########################################] | 100% Completed | 101.47 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.50%\n",
            "Max CPU Usage: 0.50%\n",
            "Min CPU Usage: 0.50%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Loading subsequent file...\n",
            "[########################################] | 100% Completed | 101.50 ms\n",
            "[########################################] | 100% Completed | 101.34 ms\n",
            "[########################################] | 100% Completed | 101.23 ms\n",
            "[########################################] | 100% Completed | 101.34 ms\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Method 5 Results:\n",
            "Initial Load: 0.49s (100 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.5%, Max CPU: 0.5%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 0.49s (50 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Truncating table for next method...\n",
            "Table truncated successfully\n",
            "\n",
            "=== Method 6: PostgreSQL COPY ===\n",
            "\n",
            "Loading initial file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.30%\n",
            "Max CPU Usage: 0.30%\n",
            "Min CPU Usage: 0.30%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Loading subsequent file...\n",
            "\n",
            "Resource Usage Statistics:\n",
            "Average CPU Usage: 0.20%\n",
            "Max CPU Usage: 0.20%\n",
            "Min CPU Usage: 0.20%\n",
            "Average Memory Usage: 1.90%\n",
            "Max Memory Usage: 1.90%\n",
            "Min Memory Usage: 1.90%\n",
            "\n",
            "Method 6 Results:\n",
            "Initial Load: 0.01s (0 records)\n",
            "Initial Load Resource Usage:\n",
            "  Avg CPU: 0.3%, Max CPU: 0.3%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Update Load: 0.01s (0 records)\n",
            "Update Load Resource Usage:\n",
            "  Avg CPU: 0.2%, Max CPU: 0.2%\n",
            "  Avg Memory: 1.9%, Max Memory: 1.9%\n",
            "\n",
            "Final Results:\n",
            "============================================================================================================================================\n",
            "Method               Initial Load Time    Initial Records Initial Avg CPU Initial Max Memory   Update Load Time     Update Records  Update Avg CPU  Update Max Memory    Total Time     \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Row-by-row           0.06s                100             0.7%            1.9%                 0.04s                50              0.6%            1.9%                 0.10s          \n",
            "Bulk Pandas          0.04s                100             2.0%            1.9%                 0.03s                50              1.1%            1.9%                 0.07s          \n",
            "Streaming Chunks     0.03s                100             0.5%            1.9%                 0.03s                50              0.2%            1.9%                 0.06s          \n",
            "Parallel Processing  0.17s                0               0.2%            1.9%                 0.16s                0               0.6%            1.9%                 0.33s          \n",
            "Dask                 0.49s                100             0.5%            1.9%                 0.49s                50              0.2%            1.9%                 0.98s          \n",
            "PostgreSQL COPY      0.01s                0               0.3%            1.9%                 0.01s                0               0.2%            1.9%                 0.01s          \n"
          ]
        }
      ]
    }
  ]
}